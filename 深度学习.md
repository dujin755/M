# 深度学习  
## 1. 预备知识  
### 1.1 入门
导入 pytorch  
```python  
import torch
```  
使用 arrange 创建行向量  
```python 
x = torch.arange(12)
x
```  
通过张量的 shape 属性访问张量的形状  
```python
x.shape
```  
检查张量的总数  
```python
x.numel()
```  
有且只改变该张量的形状，不改变其中元素  
```python
X = x.reshape(3, 4)
X
```  
* 我们可以通过 **-1** 来调用此自动计算出维度的功能  
例子：用 `x.reshape(-1,4)` 或 `x.reshape(3,-1)` 来取代 `x.reshape(3,4)`  

设置 n 个 a 行 b 列的矩阵(其中元素全为 0)  
```python
torch.zeros((n,a,b))  
```  
全为 1 时 ：  
```python
torch.ones((n,a,b))
```  
从某个特定的概率分布中随机采样来得到张量中每个元素的值  
其中的每个元素都从均值为 0 、标准差为 1 的标准高斯分布（正态分布）中随机采样  
```python
torch.randn(a,b)
```  
### 1.2 运算符  
* 求幂运算  
```python
torch exp(x)
```  
* 也可以把多个张量连结（concatenate）在一起，把它们端对端地叠起来形成一个更大的张量  
```python
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)
```  
```python
(tensor([[ 0., 1., 2., 3.],
[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.],
[ 2., 1., 4., 3.],
[ 1., 2., 3., 4.],
[ 4., 3., 2., 1.]]),
tensor([[ 0., 1., 2., 3., 2., 1., 4., 3.],
[ 4., 5., 6., 7., 1., 2., 3., 4.],
[ 8., 9., 10., 11., 4., 3., 2., 1.]]))
```  
* 也可以进行逻辑运算符的运算  
* 对张量中所有元素求和  
```python
X.sum()
```  
### 1.3 广播机制  
该机制工作机制如下 ：  
* 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状  
* 对生成的数组执行按元素操作  
例子如下：  
```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a, b
```  
```python
(tensor([[0],
         [1],
         [2]]),
tensor([[0, 1]]))
```  
当它们经行运算时，形状不匹配  
于是我们将两个矩阵广播为一个更大的 3 × 2 矩阵  
> 矩阵a将复制列  
矩阵b将复制行  
### 1.4 切片与索引  
与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是‐1  
可以指定范围以包含第一个元素和最后一个之前的元素  
还可以通过指定索引来将元素写入矩阵  
想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值  
### 1.5 转化为其他Python对象  
将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易  
```python
A = X.numpy()
B = torch.tensor(A)
type(A), type(B)
```
```python
(numpy.ndarray, torch.Tensor)
```  
要将大小为1的张量转换为Python标量，可以调用item函数或Python的内置函数  
```python
a = torch.tensor([3.5])
a, a.item(), float(a), int(a)
```  
```python
(tensor([3.5000]), 3.5, 3.5, 3)
```  
### 1.6 数据预处理  
使用pandas预处理原始数据，并将原始数据转换为张量格式的步骤  
#### 1.6.1 读取数据集  
例子 ：  
创建一个人工数据集，并存储在CSV（逗号分隔值）文件 ../data/house_tiny.csv 中  
下面将数据集按行写入CSV文件中 :
```python
import os
os.makedirs(os.path.join('..', 'data'), exist_ok=True)
data_file = os.path.join('..', 'data', 'house_tiny.csv')
with open(data_file, 'w') as f:
f.write('NumRooms,Alley,Price\n') # 列名
f.write('NA,Pave,127500\n') # 每行表示一个数据样本
f.write('2,NA,106000\n')
f.write('4,NA,178100\n')
f.write('NA,NA,140000\n')
```  
导入 pandas 包并调用 read_csv 函数 
```python 
import pandas as pd
data = pd.read_csv(data_file)
print(data)
```  
#### 1.6.2 处理缺失数据  
“NaN”项代表缺失值  
典型的方法包括插值法和删除法，插值法用一个替代值弥补缺失值，删除法则直接忽略缺失值  

通过位置索引 iloc ，将 data 分成 inputs 和 outputs ，其中前者为 data 的前两列，而后者为 data 的最后一列  
对于 inputs 中缺少的数值，用同一列的均值替换“NaN”项
```python
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = inputs.fillna(inputs.mean())
print(inputs)
```  
对于inputs中的类别值或离散值，将“NaN”视为一个类别 

由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”，pandas可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”  

巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0  
缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1  
```python
inputs = pd.get_dummies(inputs, dummy_na=True)
print(inputs)
```  
#### 1.6.3 转化为张量格式  
```python
import torch
X = torch.tensor(inputs.to_numpy(dtype=float))
y = torch.tensor(outputs.to_numpy(dtype=float))
X, y
```  
### 1.7 线性代数  
#### 1.7.1 标量  
标量由只有一个元素的张量表示  
#### 1.7.2 向量  
向量可以被视为标量值组成的列表  
人们通过一维张量表示向量  
```python
x = torch.arange(4)
x
```  
```python  
tensor([0, 1, 2, 3])
```  
可以使用下标来引用向量的任一元素，例如可以通过xi来引用第i个元素  

向量的长度通常称为向量的维度（dimension）  
与普通的Python数组一样，我们可以通过调用Python的内置len()函数来访问张量的长度  
```python
len(x)
```  
当用张量表示一个向量（只有一个轴）时，我们也可以通过.shape属性访问向量的长度  
```python
x.shape
```  
**注意** ：维度（dimension）这个词在不同上下文时往往会有不同的含义  
* 向量或轴的维度被用来表示向量或轴的长度，即向量或轴的元素数量  
* 张量的维度用来表示张量具有的轴数  
#### 1.7.3 矩阵  
正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶  
当调用函数来实例化张量时，我们可以通过指定两个分量m和n来创建一个形状为m × n的矩阵  
```python
A = torch.arange(20).reshape(5, 4)
A
```  
现在在代码中访问矩阵的转置  
```python
A.T  
```  
#### 1.7.4 张量  
就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构  
张量是描述具有任意数量轴的n维数组的通用方法  

开始处理图像时，张量将变得更加重要，图像以n维数组形式出现，其中3个轴对应于高度、宽度，以及一个通道（channel）轴，用于表示颜色通道（红色、绿色和蓝色）  
#### 1.7.5 张量算法的基本形质  
将两个相同形状的矩阵相加，
会在这两个矩阵上执行元素加法  
```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone() # 通过分配新内存，将A的一个副本分配给B
A, A + B
```  
```python
(tensor([[ 0., 1., 2., 3.],
[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.],
[12., 13., 14., 15.],
[16., 17., 18., 19.]]),
tensor([[ 0., 2., 4., 6.],
[ 8., 10., 12., 14.],
[16., 18., 20., 22.],
[24., 26., 28., 30.],
[32., 34., 36., 38.]]))
```  
两个矩阵的按元素乘法称为Hadamard积（Hadamard product）（数学符号⊙）  
```python
A * B 
```  
```python
tensor([[ 0., 1., 4., 9.],
[ 16., 25., 36., 49.],
[ 64., 81., 100., 121.],
[144., 169., 196., 225.],
[256., 289., 324., 361.]])
```  
* 将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘  
#### 1.7.6 降维  
对任意张量进行的一个有用的操作是计算其元素的和  
```python  
x = torch.arange(4, dtype=torch.float32)
x, x.sum()
```  
可以表示任意形状张量的元素和  
```python
A.shape, A.sum()
```  
默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量  
还可以指定张量沿哪一个轴来通过求和降低维度  

以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指
定axis=0  
由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失  
```python
A_sum_axis0 = A.sum(axis=0)
A_sum_axis0, A_sum_axis0.shape
```  
指定axis=1将通过汇总所有列的元素降维（轴1）  
因此，输入轴1的维数在输出形状中消失  
```python
A_sum_axis1 = A.sum(axis=1)
A_sum_axis1, A_sum_axis1.shape
```  
* 沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和  
```python
A.sum(axis=[0, 1])
```  
*平均值*  
调用函数来计算任意形状张量的平均值  
```python
A.mean(), A.sum() / A.numel()
```  
计算平均值的函数也可以沿指定轴降低张量的维度  
```python
A.mean(axis=0), A.sum(axis=0) / A.shape[0]
```  
* **非降维求和**  
有时在调用函数来计算总和或均值时保持轴数不变会很有用  
```python
sum_A = A.sum(axis=1, keepdims=True)
sum_A
```  
```
tensor([[ 6.],
        [22.],
        [38.],
        [54.],
        [70.]])
```  
由于sum_A在对每行进行求和后仍保持两个轴，可以通过广播将A除以sum_A  
```python
A / sum_A
```  
想沿某个轴计算A元素的累积总和，比如axis=0（按行计算），可以调用cumsum函数  
此函数不会沿任何轴降低输入张量的维度  
```python
A.cumsum(axis=0)
```  
#### 1.7.7 点积  
```python
torch.ones(4, dtype = torch.float32)
x, y, torch.dot(x, y)
```  
可以通过执行按元素乘法，然后进行求和来表示两个向量的点积 ：  
```python
torch.sum(x * y)
```  
#### 1.7.8 矩阵-向量积  
在代码中使用张量表示矩阵‐向量积，使用mv函数  
当为矩阵A和向量x调用torch.mv(A, x)时，会执行矩阵‐向量积  

**注意** ： A的列维数（沿轴1的长度）必须与x的维数（其长度）相同  
```python  
A.shape, x.shape, torch.mv(A, x)
```  
#### 1.7.9 矩阵-矩阵乘法  
可以将矩阵‐矩阵乘法AB看作简单地执行m次矩阵‐向量积，并将结果拼接在一起，形成一个n × m矩阵  
```python
B = torch.ones(4, 3)
torch.mm(A, B)
```  
#### 1.7.10 范数  
非正式地说，向量的范数是表示一个向量有多大  
这里考虑的大小概念不涉及维度，而是分量的大小  

在线性代数中，向量范数是将向量映射到标量的函数f  
给定任意向量x，向量范数要满足一些属性  

* 第一个性质是：如果按常数因子α缩放向量的所有元素，其范数也会按相同常数因子的绝对值缩放 ： 
```math 
f(αx) = |α|f(x)
```  
* 第二个性质是熟悉的三角不等式 :  
```math  
f(x + y) ≤ f(x) + f(y)  
```  
* 第三个性质简单地说范数必须是非负的 :  
```math
f(x) ≥ 0
```  

事实上，欧几里得距离是一个L2范数：假设n维向量x中的元素是x1, . . . , xn，其L2范数是向量元素平方和的平方根  
在代码中，我们可以按如下方式计算向量的L2范数
```python  
u = torch.tensor([3.0, -4.0])
torch.norm(u)
```  
深度学习中更经常地使用L2范数的平方，也会经常遇到L1范数，它表示为向量元素的绝对值之和 ：  
```python
torch.abs(u).sum()
```  
### 1.8 微积分  
#### 1.8.1 导数与微分  
例子 ：定义u = f(x) 如下 ：  
```python  
%matplotlib inline
import numpy as np
from matplotlib_inline import backend_inline
from d2l import torch as d2l
def f(x):
return 3 * x ** 2 - 4 * x  
```  
```python
def numerical_lim(f, x, h):
return (f(x + h) - f(x)) / h
h = 0.1
for i in range(5):
print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')
h *= 0.1
```  
可得输出结果为 ：  
```python
h=0.10000, numerical limit=2.30000
h=0.01000, numerical limit=2.03000
h=0.00100, numerical limit=2.00300
h=0.00010, numerical limit=2.00030
h=0.00001, numerical limit=2.00003
```  
#### 1.8.2 偏导数  
#### 1.8.3 梯度  
函数f(x)相对于x的梯度是一个包含n个偏导数的向量  
#### 1.8.4 链式法则  
根据链式法则 ：  
```math
dy/dx =dy/du*du/dx
```  
### 1.9 自动微分  
```python
x.requires_grad_(True) # 等价于x=torch.arange(4.0,requires_grad=True)
x.grad # 默认值是None
```  
现在计算 y 值  
```python
y = 2 * torch.dot(x, x)
y
```  
x是一个长度为4的向量，计算x和x的点积，得到了我们赋值给y的标量输出  
接下来，通过调用反向传播函数来自动计算y关于x每个分量的梯度，并打印这些梯度  
```python
y.backward()
x.grad
```  
#### 1.9.1 非标量变量的反向传播  
当y不是标量时，向量y关于向量x的导数的最自然解释是一个矩阵  
对于高阶和高维的y和x，求导的结果可以是一个高阶张量  

但当调用向量的反向计算时，通常会试图计算一批训练样本中每个组成部分的损失函数的导数  
目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和  
```python
# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。
# 本例只想求偏导数的和，所以传递一个1的梯度是合适的
x.grad.zero_()
y = x * x
# 等价于y.backward(torch.ones(len(x)))
y.sum().backward()
x.grad
```  
#### 1.9.2 分离计算  
如果希望将某些计算移动到记录的计算图之外  
想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数，并且只考虑到x在y被计算后发挥的作用  

这里可以分离y来返回一个新变量u，该变量与y具有相同的值，但丢弃计算图中计算y任何信息  
因此，下面的反向传播函数计算z=u * x关于x的偏导数，同时将u作为常数处理，而不是z=x * x * x关于x的偏导数  
```python
x.grad.zero_()
y = x * x
u = y.detach()
z = u * x
z.sum().backward()
x.grad == u
```  
由于记录了y的计算结果，我们可以随后在y上调用反向传播，得到y=x * x关于的x的导数，即2 * x  
```python
x.grad.zero_()
y.sum().backward()
x.grad == 2 * x
```  
#### 1.9.3 Python控制流的梯度计算  
使用自动微分的一个好处是：即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度  
```python
def f(a):
b = a * 2
while b.norm() < 1000:
b = b * 2
if b.sum() > 0:
c = b
else:
c = 100 * b
return c
```  
