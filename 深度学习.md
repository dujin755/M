# 深度学习  
## 1. 预备知识  
### 1.1 入门
导入 pytorch  
```python  
import torch
```  
使用 arrange 创建行向量  
```python 
x = torch.arange(12)
x
```  
通过张量的 shape 属性访问张量的形状  
```python
x.shape
```  
检查张量的总数  
```python
x.numel()
```  
有且只改变该张量的形状，不改变其中元素  
```python
X = x.reshape(3, 4)
X
```  
* 我们可以通过 **-1** 来调用此自动计算出维度的功能  
例子：用 `x.reshape(-1,4)` 或 `x.reshape(3,-1)` 来取代 `x.reshape(3,4)`  

设置 n 个 a 行 b 列的矩阵(其中元素全为 0)  
```python
torch.zeros((n,a,b))  
```  
全为 1 时 ：  
```python
torch.ones((n,a,b))
```  
从某个特定的概率分布中随机采样来得到张量中每个元素的值  
其中的每个元素都从均值为 0 、标准差为 1 的标准高斯分布（正态分布）中随机采样  
```python
torch.randn(a,b)
```  
### 1.2 运算符  
* 求幂运算  
```python
torch exp(x)
```  
* 也可以把多个张量连结（concatenate）在一起，把它们端对端地叠起来形成一个更大的张量  
```python
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)
```  
```python
(tensor([[ 0., 1., 2., 3.],
[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.],
[ 2., 1., 4., 3.],
[ 1., 2., 3., 4.],
[ 4., 3., 2., 1.]]),
tensor([[ 0., 1., 2., 3., 2., 1., 4., 3.],
[ 4., 5., 6., 7., 1., 2., 3., 4.],
[ 8., 9., 10., 11., 4., 3., 2., 1.]]))
```  
* 也可以进行逻辑运算符的运算  
* 对张量中所有元素求和  
```python
X.sum()
```  
### 1.3 广播机制  
该机制工作机制如下 ：  
* 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状  
* 对生成的数组执行按元素操作  
例子如下：  
```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a, b
```  
```python
(tensor([[0],
         [1],
         [2]]),
tensor([[0, 1]]))
```  
当它们经行运算时，形状不匹配  
于是我们将两个矩阵广播为一个更大的 3 × 2 矩阵  
> 矩阵a将复制列  
矩阵b将复制行  
### 1.4 切片与索引  
与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是‐1  
可以指定范围以包含第一个元素和最后一个之前的元素  
还可以通过指定索引来将元素写入矩阵  
想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值  
### 1.5 转化为其他Python对象  
将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易  
```python
A = X.numpy()
B = torch.tensor(A)
type(A), type(B)
```
```python
(numpy.ndarray, torch.Tensor)
```  
要将大小为1的张量转换为Python标量，可以调用item函数或Python的内置函数  
```python
a = torch.tensor([3.5])
a, a.item(), float(a), int(a)
```  
```python
(tensor([3.5000]), 3.5, 3.5, 3)
```  
### 1.6 数据预处理  
使用pandas预处理原始数据，并将原始数据转换为张量格式的步骤  
#### 1.6.1 读取数据集  
例子 ：  
创建一个人工数据集，并存储在CSV（逗号分隔值）文件 ../data/house_tiny.csv 中  
下面将数据集按行写入CSV文件中 :
```python
import os
os.makedirs(os.path.join('..', 'data'), exist_ok=True)
data_file = os.path.join('..', 'data', 'house_tiny.csv')
with open(data_file, 'w') as f:
f.write('NumRooms,Alley,Price\n') # 列名
f.write('NA,Pave,127500\n') # 每行表示一个数据样本
f.write('2,NA,106000\n')
f.write('4,NA,178100\n')
f.write('NA,NA,140000\n')
```  
导入 pandas 包并调用 read_csv 函数 
```python 
import pandas as pd
data = pd.read_csv(data_file)
print(data)
```  
#### 1.6.2 处理缺失数据  
“NaN”项代表缺失值  
典型的方法包括插值法和删除法，插值法用一个替代值弥补缺失值，删除法则直接忽略缺失值  

通过位置索引 iloc ，将 data 分成 inputs 和 outputs ，其中前者为 data 的前两列，而后者为 data 的最后一列  
对于 inputs 中缺少的数值，用同一列的均值替换“NaN”项
```python
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = inputs.fillna(inputs.mean())
print(inputs)
```  
对于inputs中的类别值或离散值，将“NaN”视为一个类别 

由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”，pandas可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”  

巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0  
缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1  
```python
inputs = pd.get_dummies(inputs, dummy_na=True)
print(inputs)
```  
#### 1.6.3 转化为张量格式  
```python
import torch
X = torch.tensor(inputs.to_numpy(dtype=float))
y = torch.tensor(outputs.to_numpy(dtype=float))
X, y
```  
### 1.7 线性代数  
#### 1.7.1 标量  
标量由只有一个元素的张量表示  
#### 1.7.2 向量  
向量可以被视为标量值组成的列表  
人们通过一维张量表示向量  
```python
x = torch.arange(4)
x
```  
```python  
tensor([0, 1, 2, 3])
```  
可以使用下标来引用向量的任一元素，例如可以通过xi来引用第i个元素  

向量的长度通常称为向量的维度（dimension）  
与普通的Python数组一样，我们可以通过调用Python的内置len()函数来访问张量的长度  
```python
len(x)
```  
当用张量表示一个向量（只有一个轴）时，我们也可以通过.shape属性访问向量的长度  
```python
x.shape
```  
**注意** ：维度（dimension）这个词在不同上下文时往往会有不同的含义  
* 向量或轴的维度被用来表示向量或轴的长度，即向量或轴的元素数量  
* 张量的维度用来表示张量具有的轴数  
#### 1.7.3 矩阵  
正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶  
当调用函数来实例化张量时，我们可以通过指定两个分量m和n来创建一个形状为m × n的矩阵  
```python
A = torch.arange(20).reshape(5, 4)
A
```  
现在在代码中访问矩阵的转置  
```python
A.T  
```  
#### 1.7.4 张量  
就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构  
张量是描述具有任意数量轴的n维数组的通用方法  

开始处理图像时，张量将变得更加重要，图像以n维数组形式出现，其中3个轴对应于高度、宽度，以及一个通道（channel）轴，用于表示颜色通道（红色、绿色和蓝色）  
#### 1.7.5 张量算法的基本形质  
将两个相同形状的矩阵相加，
会在这两个矩阵上执行元素加法  
```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone() # 通过分配新内存，将A的一个副本分配给B
A, A + B
```  
```python
(tensor([[ 0., 1., 2., 3.],
[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.],
[12., 13., 14., 15.],
[16., 17., 18., 19.]]),
tensor([[ 0., 2., 4., 6.],
[ 8., 10., 12., 14.],
[16., 18., 20., 22.],
[24., 26., 28., 30.],
[32., 34., 36., 38.]]))
```  
两个矩阵的按元素乘法称为Hadamard积（Hadamard product）（数学符号⊙）  
```python
A * B 
```  
```python
tensor([[ 0., 1., 4., 9.],
[ 16., 25., 36., 49.],
[ 64., 81., 100., 121.],
[144., 169., 196., 225.],
[256., 289., 324., 361.]])
```  
* 将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘  
#### 1.7.6 降维  
对任意张量进行的一个有用的操作是计算其元素的和  
```python  
x = torch.arange(4, dtype=torch.float32)
x, x.sum()
```  
可以表示任意形状张量的元素和  
```python
A.shape, A.sum()
```  
默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量  
还可以指定张量沿哪一个轴来通过求和降低维度  

以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指
定axis=0  
由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失  
```python
A_sum_axis0 = A.sum(axis=0)
A_sum_axis0, A_sum_axis0.shape
```  
指定axis=1将通过汇总所有列的元素降维（轴1）  
因此，输入轴1的维数在输出形状中消失  
```python
A_sum_axis1 = A.sum(axis=1)
A_sum_axis1, A_sum_axis1.shape
```  
* 沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和  
```python
A.sum(axis=[0, 1])
```  
*平均值*  
调用函数来计算任意形状张量的平均值  
```python
A.mean(), A.sum() / A.numel()
```  
计算平均值的函数也可以沿指定轴降低张量的维度  
```python
A.mean(axis=0), A.sum(axis=0) / A.shape[0]
```  
* **非降维求和**  
有时在调用函数来计算总和或均值时保持轴数不变会很有用  
```python
sum_A = A.sum(axis=1, keepdims=True)
sum_A
```  
```
tensor([[ 6.],
        [22.],
        [38.],
        [54.],
        [70.]])
```  
由于sum_A在对每行进行求和后仍保持两个轴，可以通过广播将A除以sum_A  
```python
A / sum_A
```  
想沿某个轴计算A元素的累积总和，比如axis=0（按行计算），可以调用cumsum函数  
此函数不会沿任何轴降低输入张量的维度  
```python
A.cumsum(axis=0)
```  
#### 1.7.7 点积  
```python
torch.ones(4, dtype = torch.float32)
x, y, torch.dot(x, y)
```  
可以通过执行按元素乘法，然后进行求和来表示两个向量的点积 ：  
```python
torch.sum(x * y)
```  
#### 1.7.8 矩阵-向量积  
在代码中使用张量表示矩阵‐向量积，使用mv函数  
当为矩阵A和向量x调用torch.mv(A, x)时，会执行矩阵‐向量积  

**注意** ： A的列维数（沿轴1的长度）必须与x的维数（其长度）相同  
```python  
A.shape, x.shape, torch.mv(A, x)
```  
#### 1.7.9 矩阵-矩阵乘法  
可以将矩阵‐矩阵乘法AB看作简单地执行m次矩阵‐向量积，并将结果拼接在一起，形成一个n × m矩阵  
```python
B = torch.ones(4, 3)
torch.mm(A, B)
```  
#### 1.7.10 范数  
非正式地说，向量的范数是表示一个向量有多大  
这里考虑的大小概念不涉及维度，而是分量的大小  

在线性代数中，向量范数是将向量映射到标量的函数f  
给定任意向量x，向量范数要满足一些属性  

* 第一个性质是：如果按常数因子α缩放向量的所有元素，其范数也会按相同常数因子的绝对值缩放 ： 
```math 
f(αx) = |α|f(x)
```  
* 第二个性质是熟悉的三角不等式 :  
```math  
f(x + y) ≤ f(x) + f(y)  
```  
* 第三个性质简单地说范数必须是非负的 :  
```math
f(x) ≥ 0
```  

事实上，欧几里得距离是一个L2范数：假设n维向量x中的元素是x1, . . . , xn，其L2范数是向量元素平方和的平方根  
在代码中，我们可以按如下方式计算向量的L2范数
```python  
u = torch.tensor([3.0, -4.0])
torch.norm(u)
```  
深度学习中更经常地使用L2范数的平方，也会经常遇到L1范数，它表示为向量元素的绝对值之和 ：  
```python
torch.abs(u).sum()
```  
### 1.8 微积分  
#### 1.8.1 导数与微分  
例子 ：定义u = f(x) 如下 ：  
```python  
%matplotlib inline
import numpy as np
from matplotlib_inline import backend_inline
from d2l import torch as d2l
def f(x):
return 3 * x ** 2 - 4 * x  
```  
```python
def numerical_lim(f, x, h):
return (f(x + h) - f(x)) / h
h = 0.1
for i in range(5):
print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')
h *= 0.1
```  
可得输出结果为 ：  
```python
h=0.10000, numerical limit=2.30000
h=0.01000, numerical limit=2.03000
h=0.00100, numerical limit=2.00300
h=0.00010, numerical limit=2.00030
h=0.00001, numerical limit=2.00003
```  
#### 1.8.2 偏导数  
#### 1.8.3 梯度  
函数f(x)相对于x的梯度是一个包含n个偏导数的向量  
#### 1.8.4 链式法则  
根据链式法则 ：  
```math
dy/dx =dy/du*du/dx
```  
### 1.9 自动微分  
```python
x.requires_grad_(True) # 等价于x=torch.arange(4.0,requires_grad=True)
x.grad # 默认值是None
```  
现在计算 y 值  
```python
y = 2 * torch.dot(x, x)
y
```  
x是一个长度为4的向量，计算x和x的点积，得到了我们赋值给y的标量输出  
接下来，通过调用反向传播函数来自动计算y关于x每个分量的梯度，并打印这些梯度  
```python
y.backward()
x.grad
```  
#### 1.9.1 非标量变量的反向传播  
当y不是标量时，向量y关于向量x的导数的最自然解释是一个矩阵  
对于高阶和高维的y和x，求导的结果可以是一个高阶张量  

但当调用向量的反向计算时，通常会试图计算一批训练样本中每个组成部分的损失函数的导数  
目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和  
```python
# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。
# 本例只想求偏导数的和，所以传递一个1的梯度是合适的
x.grad.zero_()
y = x * x
# 等价于y.backward(torch.ones(len(x)))
y.sum().backward()
x.grad
```  
#### 1.9.2 分离计算  
如果希望将某些计算移动到记录的计算图之外  
想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数，并且只考虑到x在y被计算后发挥的作用  

这里可以分离y来返回一个新变量u，该变量与y具有相同的值，但丢弃计算图中计算y任何信息  
因此，下面的反向传播函数计算z=u * x关于x的偏导数，同时将u作为常数处理，而不是z=x * x * x关于x的偏导数  
```python
x.grad.zero_()
y = x * x
u = y.detach()
z = u * x
z.sum().backward()
x.grad == u
```  
由于记录了y的计算结果，我们可以随后在y上调用反向传播，得到y=x * x关于的x的导数，即2 * x  
```python
x.grad.zero_()
y.sum().backward()
x.grad == 2 * x
```  
#### 1.9.3 Python控制流的梯度计算  
使用自动微分的一个好处是：即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度  
```python
def f(a):
b = a * 2
while b.norm() < 1000:
b = b * 2
if b.sum() > 0:
c = b
else:
c = 100 * b
return c
```  
计算梯度 ：  
```python  
a = torch.randn(size=(), requires_grad=True)
d = f(a)
d.backward()
```  

## 2. 线性神经网络  
### 2.1 线性回归  
回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法  
#### 2.1.1 线性回归的基本元素  
线性回归基于几个简单的假设：首先，假设自变量 **x** 和因变量 y 之间的关系是线性的，即 y 可以表示为 **x** 中元素的加权和，这里通常允许包含观测值的一些噪声；其次，我们假设任何噪声都比较正常，如噪声遵循正态分布  

**线性假设**  
以一个房屋的案例 ：  
线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和，如下面的式子 ：  
```math
price = warea · area + wage · age + b.  
```  
warea和wage 称为权重（weight），权重决定了每个特征对我们预测值的影响  
b称为偏置（bias）、偏移量（offset）或截距（intercept）  
偏置是指当所有特征都取值为0时，预测值应该为多少  

给定一个数据集，我们的目标是寻找模型的权重 **w** 和偏置 b ，使得根据模型做出的预测大体符合数据里的真实价格  

而在机器学习领域，通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。输入包含d个特征时，我们将预测结果yˆ （通常使用“尖角”符号表示y的估计值）表示为 :  
```math
yˆ = w1x1 + ... + wdxd + b.  
```  
将所有特征放到向量**x** ∈ Rd中，并将所有权重放到向量**w** ∈ Rd中，我们可以用点积形式来简洁地表达模型 ：  
```math
yˆ = w⊤x + b.
```  
**损失函数**  
开始考虑如何用模型拟合（fit）数据之前，需要确定一个拟合程度的度量  
损失函数（loss function）能够量化目标的实际值与预测值之间的差距  

通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0  
回归问题中最常用的损失函数是平方误差函数  
当样本i的预测值为yˆ(i)，其相应的真实标签为y(i)时，平方误差可以定义为以下公式 ：  
$$
l^i(w,b)={(\hat y^i -y^i)^2/2}
$$  
在训练模型时，我们希望寻找一组参数（w∗, b∗），这组参数能最小化在所有训练样本上的总损失  

**解析解**  
线性回归的解可以用一个公式简单地表达出来，这类解叫作解析解（analytical solution）  

***随机梯度下降***  
即便在无法得到解析解，仍然可以有效地训练模型  
用一种名为梯度下降（gradient descent）的方法，这种方法几乎可以优化所有深度学习模型  
梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可以称为梯度），但是这种方式在每一次更新参数时会遍历一遍数据库
因此，通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做小批量随机梯度下降  

算法的步骤如下：（1）初始化模型参数的值，如随机初始化；（2）从数据集中随机抽取小批量样
本且在负梯度的方向上更新参数，并不断迭代这一步骤  

在训练了预先确定的若干迭代次数后（或者直到满足某些其他停止条件后），记录下模型参数的估计值，表示为
$$\hat w ,\hat b$$  
但是即使函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值  
因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值  
### 2.2 softmax 运算  
#### 2.2.1 网络架构  
为了估计所有可能类别的条件概率，需要一个有多个输出的模型，每个类别对应一个输出  
为了解决线性模型的分类问题，需要和输出一样多的仿射函数  

例子中，由于有4个特征和3个可能的输出类别   
将需要12个标量来表示权重  
3个标量来表示偏置 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true" columnalign="right" columnspacing="0em" rowspacing="3pt">
    <mtr>
      <mtd>
        <mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
          <mtr>
            <mtd>
              <msub>
                <mi>o</mi>
                <mn>1</mn>
              </msub>
            </mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <msub>
                <mi>x</mi>
                <mn>1</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>11</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>2</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>12</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>3</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>13</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>4</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>14</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>b</mi>
                <mn>1</mn>
              </msub>
              <mo>,</mo>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <msub>
                <mi>o</mi>
                <mn>2</mn>
              </msub>
            </mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <msub>
                <mi>x</mi>
                <mn>1</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>21</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>2</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>22</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>3</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>23</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>4</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>24</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>b</mi>
                <mn>2</mn>
              </msub>
              <mo>,</mo>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <msub>
                <mi>o</mi>
                <mn>3</mn>
              </msub>
            </mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <msub>
                <mi>x</mi>
                <mn>1</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>31</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>2</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>32</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>3</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>33</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>4</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>34</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>b</mi>
                <mn>3</mn>
              </msub>
              <mo>.</mo>
            </mtd>
          </mtr>
        </mtable>
      </mtd>
    </mtr>
  </mtable>
</math>  
可以用神经网络图 图3.4.1来描述这个计算过程  
与线性回归一样，softmax回归也是一个单层神经网络 :  
![图像](https://zh.d2l.ai/_images/softmaxreg.svg)  
通过向量形式表达为<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">o</mi>
  </mrow>
  <mo>=</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">W</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo>+</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">b</mi>
  </mrow>
</math>这是一种更适合数学和编写代码的形式  
已经将所有权重放到一个3*4矩阵中  

#### 2.2.2 softmax 运算  
为了得到预测结果，将设置一个阈值，如选择具有最大概率的标签  
要将输出视为概率，必须保证在任何数据上的输出都是非负的且总和为1  
需要一个训练的目标函数，来激励模型精准地估计概率  

softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质  
为了完成这一目标，首先对每个未规范化的预测求幂，这样可以确保输出非负  
为了确保最终输出的概率值总和为1，再让每个求幂后的结果除以它们的总和，如下式 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow data-mjx-texclass="ORD">
    <mover>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">y</mi>
      </mrow>
      <mo stretchy="false">^</mo>
    </mover>
  </mrow>
  <mo>=</mo>
  <mrow data-mjx-texclass="ORD">
    <mi data-mjx-auto-op="false">softmax</mi>
  </mrow>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">o</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mstyle scriptlevel="0">
    <mspace width="1em"></mspace>
  </mstyle>
  <mtext>&#x5176;&#x4E2D;</mtext>
  <mstyle scriptlevel="0">
    <mspace width="1em"></mspace>
  </mstyle>
  <msub>
    <mrow data-mjx-texclass="ORD">
      <mover>
        <mi>y</mi>
        <mo stretchy="false">^</mo>
      </mover>
    </mrow>
    <mi>j</mi>
  </msub>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>o</mi>
        <mi>j</mi>
      </msub>
      <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
      <munder>
        <mo data-mjx-texclass="OP">&#x2211;</mo>
        <mi>k</mi>
      </munder>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>o</mi>
        <mi>k</mi>
      </msub>
      <mo stretchy="false">)</mo>
    </mrow>
  </mfrac>
</math>  

#### 2.2.3 损失函数  
需要一个损失函数来度量预测的效果  
将使用最大似然估计  
##### 2.2.3.1 对数似然  
softmax函数给出了一个向量 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mover>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">y</mi>
      </mrow>
      <mo stretchy="false">^</mo>
    </mover>
  </mrow>
</math> 可以将其视为“对给定任意输入 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
</math> 的每个类的条件概率”  

可以将估计值与实际值进行比较 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>P</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">Y</mi>
  </mrow>
  <mo>&#x2223;</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">X</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <munderover>
    <mo data-mjx-texclass="OP">&#x220F;</mo>
    <mrow data-mjx-texclass="ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>n</mi>
  </munderover>
  <mi>P</mi>
  <mo stretchy="false">(</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">y</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>&#x2223;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">x</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo stretchy="false">)</mo>
  <mo>.</mo>
</math>   
根据最大似然估计，最大化 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>P</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">Y</mi>
  </mrow>
  <mo>&#x2223;</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">X</mi>
  </mrow>
  <mo stretchy="false">)</mo>
</math> 相当于最小化负对数似然 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mo>&#x2212;</mo>
  <mi>log</mi>
  <mo data-mjx-texclass="NONE">&#x2061;</mo>
  <mi>P</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">Y</mi>
  </mrow>
  <mo>&#x2223;</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">X</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <munderover>
    <mo data-mjx-texclass="OP">&#x2211;</mo>
    <mrow data-mjx-texclass="ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>n</mi>
  </munderover>
  <mo>&#x2212;</mo>
  <mi>log</mi>
  <mo data-mjx-texclass="NONE">&#x2061;</mo>
  <mi>P</mi>
  <mo stretchy="false">(</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">y</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>&#x2223;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">x</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <munderover>
    <mo data-mjx-texclass="OP">&#x2211;</mo>
    <mrow data-mjx-texclass="ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>n</mi>
  </munderover>
  <mi>l</mi>
  <mo stretchy="false">(</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">y</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>,</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mover>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">y</mi>
        </mrow>
        <mo stretchy="false">^</mo>
      </mover>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo stretchy="false">)</mo>
  <mo>,</mo>
</math>  
其中，对于任何标签 **y** 和预测模型 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mover>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">y</mi>
      </mrow>
      <mo stretchy="false">^</mo>
    </mover>
  </mrow>
</math> 损失函数为 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>l</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">y</mi>
  </mrow>
  <mo>,</mo>
  <mrow data-mjx-texclass="ORD">
    <mover>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">y</mi>
      </mrow>
      <mo stretchy="false">^</mo>
    </mover>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mo>&#x2212;</mo>
  <munderover>
    <mo data-mjx-texclass="OP">&#x2211;</mo>
    <mrow data-mjx-texclass="ORD">
      <mi>j</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>q</mi>
  </munderover>
  <msub>
    <mi>y</mi>
    <mi>j</mi>
  </msub>
  <mi>log</mi>
  <mo data-mjx-texclass="NONE">&#x2061;</mo>
  <msub>
    <mrow data-mjx-texclass="ORD">
      <mover>
        <mi>y</mi>
        <mo stretchy="false">^</mo>
      </mover>
    </mrow>
    <mi>j</mi>
  </msub>
  <mo>.</mo>
</math>  
该类损失函数通常被称为交叉熵损失函数  
##### 2.2.3.2 softmax 及其导数  
利用softmax的定义，我们得到 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true" columnalign="right" columnspacing="0em" rowspacing="3pt">
    <mtr>
      <mtd>
        <mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
          <mtr>
            <mtd>
              <mi>l</mi>
              <mo stretchy="false">(</mo>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">y</mi>
              </mrow>
              <mo>,</mo>
              <mrow data-mjx-texclass="ORD">
                <mover>
                  <mrow data-mjx-texclass="ORD">
                    <mi mathvariant="bold">y</mi>
                  </mrow>
                  <mo stretchy="false">^</mo>
                </mover>
              </mrow>
              <mo stretchy="false">)</mo>
            </mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <mo>&#x2212;</mo>
              <munderover>
                <mo data-mjx-texclass="OP">&#x2211;</mo>
                <mrow data-mjx-texclass="ORD">
                  <mi>j</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>q</mi>
              </munderover>
              <msub>
                <mi>y</mi>
                <mi>j</mi>
              </msub>
              <mi>log</mi>
              <mo data-mjx-texclass="NONE">&#x2061;</mo>
              <mfrac>
                <mrow>
                  <mi>exp</mi>
                  <mo data-mjx-texclass="NONE">&#x2061;</mo>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mi>o</mi>
                    <mi>j</mi>
                  </msub>
                  <mo stretchy="false">)</mo>
                </mrow>
                <mrow>
                  <munderover>
                    <mo data-mjx-texclass="OP">&#x2211;</mo>
                    <mrow data-mjx-texclass="ORD">
                      <mi>k</mi>
                      <mo>=</mo>
                      <mn>1</mn>
                    </mrow>
                    <mi>q</mi>
                  </munderover>
                  <mi>exp</mi>
                  <mo data-mjx-texclass="NONE">&#x2061;</mo>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mi>o</mi>
                    <mi>k</mi>
                  </msub>
                  <mo stretchy="false">)</mo>
                </mrow>
              </mfrac>
            </mtd>
          </mtr>
          <mtr>
            <mtd></mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <munderover>
                <mo data-mjx-texclass="OP">&#x2211;</mo>
                <mrow data-mjx-texclass="ORD">
                  <mi>j</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>q</mi>
              </munderover>
              <msub>
                <mi>y</mi>
                <mi>j</mi>
              </msub>
              <mi>log</mi>
              <mo data-mjx-texclass="NONE">&#x2061;</mo>
              <munderover>
                <mo data-mjx-texclass="OP">&#x2211;</mo>
                <mrow data-mjx-texclass="ORD">
                  <mi>k</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>q</mi>
              </munderover>
              <mi>exp</mi>
              <mo data-mjx-texclass="NONE">&#x2061;</mo>
              <mo stretchy="false">(</mo>
              <msub>
                <mi>o</mi>
                <mi>k</mi>
              </msub>
              <mo stretchy="false">)</mo>
              <mo>&#x2212;</mo>
              <munderover>
                <mo data-mjx-texclass="OP">&#x2211;</mo>
                <mrow data-mjx-texclass="ORD">
                  <mi>j</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>q</mi>
              </munderover>
              <msub>
                <mi>y</mi>
                <mi>j</mi>
              </msub>
              <msub>
                <mi>o</mi>
                <mi>j</mi>
              </msub>
            </mtd>
          </mtr>
          <mtr>
            <mtd></mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <mi>log</mi>
              <mo data-mjx-texclass="NONE">&#x2061;</mo>
              <munderover>
                <mo data-mjx-texclass="OP">&#x2211;</mo>
                <mrow data-mjx-texclass="ORD">
                  <mi>k</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>q</mi>
              </munderover>
              <mi>exp</mi>
              <mo data-mjx-texclass="NONE">&#x2061;</mo>
              <mo stretchy="false">(</mo>
              <msub>
                <mi>o</mi>
                <mi>k</mi>
              </msub>
              <mo stretchy="false">)</mo>
              <mo>&#x2212;</mo>
              <munderover>
                <mo data-mjx-texclass="OP">&#x2211;</mo>
                <mrow data-mjx-texclass="ORD">
                  <mi>j</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>q</mi>
              </munderover>
              <msub>
                <mi>y</mi>
                <mi>j</mi>
              </msub>
              <msub>
                <mi>o</mi>
                <mi>j</mi>
              </msub>
              <mo>.</mo>
            </mtd>
          </mtr>
        </mtable>
      </mtd>
    </mtr>
  </mtable>
</math>  
考虑相对于任何未规范化的预测 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msub>
    <mi>o</mi>
    <mi>j</mi>
  </msub>
</math> 的导数，我们得到 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <msub>
    <mi>&#x2202;</mi>
    <mrow data-mjx-texclass="ORD">
      <msub>
        <mi>o</mi>
        <mi>j</mi>
      </msub>
    </mrow>
  </msub>
  <mi>l</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">y</mi>
  </mrow>
  <mo>,</mo>
  <mrow data-mjx-texclass="ORD">
    <mover>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">y</mi>
      </mrow>
      <mo stretchy="false">^</mo>
    </mover>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>o</mi>
        <mi>j</mi>
      </msub>
      <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
      <munderover>
        <mo data-mjx-texclass="OP">&#x2211;</mo>
        <mrow data-mjx-texclass="ORD">
          <mi>k</mi>
          <mo>=</mo>
          <mn>1</mn>
        </mrow>
        <mi>q</mi>
      </munderover>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>o</mi>
        <mi>k</mi>
      </msub>
      <mo stretchy="false">)</mo>
    </mrow>
  </mfrac>
  <mo>&#x2212;</mo>
  <msub>
    <mi>y</mi>
    <mi>j</mi>
  </msub>
  <mo>=</mo>
  <mrow data-mjx-texclass="ORD">
    <mi data-mjx-auto-op="false">softmax</mi>
  </mrow>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">o</mi>
  </mrow>
  <msub>
    <mo stretchy="false">)</mo>
    <mi>j</mi>
  </msub>
  <mo>&#x2212;</mo>
  <msub>
    <mi>y</mi>
    <mi>j</mi>
  </msub>
  <mo>.</mo>
</math>  

##### 2.2.3.3 交叉熵损失  
现在用一个概率向量表示，如 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mo stretchy="false">(</mo>
  <mn>0.1</mn>
  <mo>,</mo>
  <mn>0.2</mn>
  <mo>,</mo>
  <mn>0.7</mn>
  <mo stretchy="false">)</mo>
</math> ，而不是仅包含二元项 (0,0,1) 的向量  
定义一个损失 *l* ,它是所有标签分布的预期损失值  
此损失被称为**交叉熵损失**，它是分类问题最常用的损失之一  

## 3. 多层感知机  
### 3.1 多层感知机  
#### 3.1.1 隐藏层  
在之前章节中描述了仿射变换，它是一种带偏置的线性变化  
该模型通过单个仿射变换将我们的输入直接映射到输出，然后进行softmax操作  
如果标签通过仿射变换后确实与输入数据相关，那么这种方法确实足够了  
但是仿射中的线性是一种很强的假设  
##### 3.1.1.1 线性变换的局限性  
线性意味着单调假设  
很容易找到违反单调性的假设 ：例如根据人的体温预测死亡率  
##### 3.1.1.2 在网络中加入隐藏层  
可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型  
要做到这一点，最简单的方法是将许多全连接层堆叠在一起  
这种架构通常称为多层感知机，通常缩写为MLP  
![图片](https://zh.d2l.ai/_images/mlp.svg)  
注意，这两个层都是全连接的  
每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元  
##### 3.1.1.3 从线性到非线性  
同之前的章节一样， 我们通过矩阵<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">X</mi>
  </mrow>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>n</mi>
      <mo>&#xD7;</mo>
      <mi>d</mi>
    </mrow>
  </msup>
</math>来表示 *n* 个样本的小批量，其中每个样本具有个输入 *d* 特征  
对于具有 *h* 个隐藏单元的单隐藏层多层感知机，用<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">H</mi>
  </mrow>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>n</mi>
      <mo>&#xD7;</mo>
      <mi>h</mi>
    </mrow>
  </msup>
</math>表示隐藏层的输出，称为隐藏表示  
因为隐藏层和输出层都是全连接的， 所以我们有隐藏层权重<math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>d</mi>
      <mo>&#xD7;</mo>
      <mi>h</mi>
    </mrow>
  </msup>
</math> 和隐藏层偏置<math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">b</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mn>1</mn>
      <mo>&#xD7;</mo>
      <mi>h</mi>
    </mrow>
  </msup>
</math> 以及输出层权重<math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>h</mi>
      <mo>&#xD7;</mo>
      <mi>q</mi>
    </mrow>
  </msup>
</math> 和输出层偏置<math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">b</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mn>1</mn>
      <mo>&#xD7;</mo>
      <mi>q</mi>
    </mrow>
  </msup>
</math>  
按如下方式计算单隐藏层多层感知机的输出 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">O</mi>
  </mrow>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>n</mi>
      <mo>&#xD7;</mo>
      <mi>q</mi>
    </mrow>
  </msup>
</math>  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true" columnalign="right" columnspacing="0em" rowspacing="3pt">
    <mtr>
      <mtd>
        <mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
          <mtr>
            <mtd>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">H</mi>
              </mrow>
            </mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">X</mi>
              </mrow>
              <msup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="bold">W</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">(</mo>
                  <mn>1</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="bold">b</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">(</mo>
                  <mn>1</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>,</mo>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">O</mi>
              </mrow>
            </mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">H</mi>
              </mrow>
              <msup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="bold">W</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">(</mo>
                  <mn>2</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="bold">b</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">(</mo>
                  <mn>2</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>.</mo>
            </mtd>
          </mtr>
        </mtable>
      </mtd>
    </mtr>
  </mtable>
</math>  
合并隐藏层，便可产生具有参数<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">W</mi>
  </mrow>
  <mo>=</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
</math> 和<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">b</mi>
  </mrow>
  <mo>=</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">b</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>+</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">b</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
</math> 的等价单层模型 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">O</mi>
  </mrow>
  <mo>=</mo>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">X</mi>
  </mrow>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>+</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">b</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo stretchy="false">)</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>+</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">b</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>=</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">X</mi>
  </mrow>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>+</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">b</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>+</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">b</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>=</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">X</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">W</mi>
  </mrow>
  <mo>+</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">b</mi>
  </mrow>
  <mo>.</mo>
</math>  
为了发挥多层架构的潜力，还需要一个额外的关键要素：在仿射变换之后对每个隐藏单元应用非线性的激活函数 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x3C3;</mi>
</math> 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true" columnalign="right" columnspacing="0em" rowspacing="3pt">
    <mtr>
      <mtd>
        <mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
          <mtr>
            <mtd>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">H</mi>
              </mrow>
            </mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <mi>&#x3C3;</mi>
              <mo stretchy="false">(</mo>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">X</mi>
              </mrow>
              <msup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="bold">W</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">(</mo>
                  <mn>1</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="bold">b</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">(</mo>
                  <mn>1</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo stretchy="false">)</mo>
              <mo>,</mo>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">O</mi>
              </mrow>
            </mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">H</mi>
              </mrow>
              <msup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="bold">W</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">(</mo>
                  <mn>2</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="bold">b</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">(</mo>
                  <mn>2</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>.</mo>
            </mtd>
          </mtr>
        </mtable>
      </mtd>
    </mtr>
  </mtable>
</math>  

#### 3.1.2 激活函数  
*激活函数* 通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算  
##### 3.1.2.1 ReLU 函数  
最受欢迎的激活函数是修正线性单元  
因为它实现简单，同时在各种预测任务中表现良好  
给定元素 **x** ，ReLU函数被定义为该元素与的最大值 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>ReLU</mi>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mo data-mjx-texclass="OP" movablelimits="true">max</mo>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo>,</mo>
  <mn>0</mn>
  <mo stretchy="false">)</mo>
  <mo>.</mo>
</math>  
可见图像为 ：  

![图片](https://zh.d2l.ai/_images/output_mlp_76f463_18_1.svg)  
ReLU函数的导数图像 ：  

![图像](https://zh.d2l.ai/_images/output_mlp_76f463_33_1.svg)  
使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过  
并且ReLU减轻了困扰以往神经网络的梯度消失问题  
ReLU函数有许多变体，包括参数化ReLU函数   
该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>pReLU</mi>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mo data-mjx-texclass="OP" movablelimits="true">max</mo>
  <mo stretchy="false">(</mo>
  <mn>0</mn>
  <mo>,</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>+</mo>
  <mi>&#x3B1;</mi>
  <mo data-mjx-texclass="OP" movablelimits="true">min</mo>
  <mo stretchy="false">(</mo>
  <mn>0</mn>
  <mo>,</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>.</mo>
</math>  

##### 3.1.2.2 sigmoid 函数   
对于一个定义域在 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="double-struck">R</mi>
  </mrow>
</math> 中的输入， sigmoid函数将输入变换为区间(0, 1)上的输出  
sigmoid通常称为挤压函数  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>sigmoid</mi>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mfrac>
    <mn>1</mn>
    <mrow>
      <mn>1</mn>
      <mo>+</mo>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <mo>&#x2212;</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </mfrac>
  <mo>.</mo>
</math>  

想要将输出视作二元分类问题的概率时， sigmoid仍然被广泛用作输出单元上的激活函数  
该函数图像为 ：  

![图片](https://zh.d2l.ai/_images/output_mlp_76f463_48_0.svg)  
igmoid函数的导数为下面的公式 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mfrac>
    <mi>d</mi>
    <mrow>
      <mi>d</mi>
      <mi>x</mi>
    </mrow>
  </mfrac>
  <mi>sigmoid</mi>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <mo>&#x2212;</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo>+</mo>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <mo>&#x2212;</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
      <msup>
        <mo stretchy="false">)</mo>
        <mn>2</mn>
      </msup>
    </mrow>
  </mfrac>
  <mo>=</mo>
  <mi>sigmoid</mi>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">(</mo>
    <mn>1</mn>
    <mo>&#x2212;</mo>
    <mi>sigmoid</mi>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
    <mo data-mjx-texclass="CLOSE">)</mo>
  </mrow>
  <mo>.</mo>
</math>  

sigmoid函数的导数图像如下所示 ：  

![图片](https://zh.d2l.ai/_images/output_mlp_76f463_63_0.svg)  
这是会发现，当函数越是远离 0 点时，导数越接近 0 ， 该现象即使“梯度消失现象”  
##### 3.1.2.3 tanh 函数  
与sigmoid函数类似， tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>tanh</mi>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mn>1</mn>
      <mo>&#x2212;</mo>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <mo>&#x2212;</mo>
      <mn>2</mn>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
      <mn>1</mn>
      <mo>+</mo>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <mo>&#x2212;</mo>
      <mn>2</mn>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </mfrac>
  <mo>.</mo>
</math>  
图像如下 ： 

![图片](https://zh.d2l.ai/_images/output_mlp_76f463_78_0.svg)  

tanh 的导数为  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mfrac>
    <mi>d</mi>
    <mrow>
      <mi>d</mi>
      <mi>x</mi>
    </mrow>
  </mfrac>
  <mi>tanh</mi>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mn>1</mn>
  <mo>&#x2212;</mo>
  <msup>
    <mi>tanh</mi>
    <mn>2</mn>
  </msup>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>.</mo>
</math>  

tanh函数的导数图像如下所示，与sigmoid 函数类似  

![图片](https://zh.d2l.ai/_images/output_mlp_76f463_93_0.svg)  

### 3.2 模型选择、欠拟合与过拟合  
#### 3.2.1 训练误差与泛化误差  
*训练误差* 是指， 模型在训练数据集上计算得到的误差  
*泛化误差* 是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望