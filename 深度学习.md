# 深度学习  
## 1. 预备知识  
### 1.1 入门
导入 pytorch  
```python  
import torch
```  
使用 arrange 创建行向量  
```python 
x = torch.arange(12)
x
```  
通过张量的 shape 属性访问张量的形状  
```python
x.shape
```  
检查张量的总数  
```python
x.numel()
```  
有且只改变该张量的形状，不改变其中元素  
```python
X = x.reshape(3, 4)
X
```  
* 我们可以通过 **-1** 来调用此自动计算出维度的功能  
例子：用 `x.reshape(-1,4)` 或 `x.reshape(3,-1)` 来取代 `x.reshape(3,4)`  

设置 n 个 a 行 b 列的矩阵(其中元素全为 0)  
```python
torch.zeros((n,a,b))  
```  
全为 1 时 ：  
```python
torch.ones((n,a,b))
```  
从某个特定的概率分布中随机采样来得到张量中每个元素的值  
其中的每个元素都从均值为 0 、标准差为 1 的标准高斯分布（正态分布）中随机采样  
```python
torch.randn(a,b)
```  
### 1.2 运算符  
* 求幂运算  
```python
torch exp(x)
```  
* 也可以把多个张量连结（concatenate）在一起，把它们端对端地叠起来形成一个更大的张量  
```python
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])
torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)
```  
```python
(tensor([[ 0., 1., 2., 3.],
[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.],
[ 2., 1., 4., 3.],
[ 1., 2., 3., 4.],
[ 4., 3., 2., 1.]]),
tensor([[ 0., 1., 2., 3., 2., 1., 4., 3.],
[ 4., 5., 6., 7., 1., 2., 3., 4.],
[ 8., 9., 10., 11., 4., 3., 2., 1.]]))
```  
* 也可以进行逻辑运算符的运算  
* 对张量中所有元素求和  
```python
X.sum()
```  
### 1.3 广播机制  
该机制工作机制如下 ：  
* 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状  
* 对生成的数组执行按元素操作  
例子如下：  
```python
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))
a, b
```  
```python
(tensor([[0],
         [1],
         [2]]),
tensor([[0, 1]]))
```  
当它们经行运算时，形状不匹配  
于是我们将两个矩阵广播为一个更大的 3 × 2 矩阵  
> 矩阵a将复制列  
矩阵b将复制行  
### 1.4 切片与索引  
与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是‐1  
可以指定范围以包含第一个元素和最后一个之前的元素  
还可以通过指定索引来将元素写入矩阵  
想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值  
### 1.5 转化为其他Python对象  
将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易  
```python
A = X.numpy()
B = torch.tensor(A)
type(A), type(B)
```
```python
(numpy.ndarray, torch.Tensor)
```  
要将大小为1的张量转换为Python标量，可以调用item函数或Python的内置函数  
```python
a = torch.tensor([3.5])
a, a.item(), float(a), int(a)
```  
```python
(tensor([3.5000]), 3.5, 3.5, 3)
```  
### 1.6 数据预处理  
使用pandas预处理原始数据，并将原始数据转换为张量格式的步骤  
#### 1.6.1 读取数据集  
例子 ：  
创建一个人工数据集，并存储在CSV（逗号分隔值）文件 ../data/house_tiny.csv 中  
下面将数据集按行写入CSV文件中 :
```python
import os
os.makedirs(os.path.join('..', 'data'), exist_ok=True)
data_file = os.path.join('..', 'data', 'house_tiny.csv')
with open(data_file, 'w') as f:
f.write('NumRooms,Alley,Price\n') # 列名
f.write('NA,Pave,127500\n') # 每行表示一个数据样本
f.write('2,NA,106000\n')
f.write('4,NA,178100\n')
f.write('NA,NA,140000\n')
```  
导入 pandas 包并调用 read_csv 函数 
```python 
import pandas as pd
data = pd.read_csv(data_file)
print(data)
```  
#### 1.6.2 处理缺失数据  
“NaN”项代表缺失值  
典型的方法包括插值法和删除法，插值法用一个替代值弥补缺失值，删除法则直接忽略缺失值  

通过位置索引 iloc ，将 data 分成 inputs 和 outputs ，其中前者为 data 的前两列，而后者为 data 的最后一列  
对于 inputs 中缺少的数值，用同一列的均值替换“NaN”项
```python
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = inputs.fillna(inputs.mean())
print(inputs)
```  
对于inputs中的类别值或离散值，将“NaN”视为一个类别 

由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”，pandas可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”  

巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0  
缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1  
```python
inputs = pd.get_dummies(inputs, dummy_na=True)
print(inputs)
```  
#### 1.6.3 转化为张量格式  
```python
import torch
X = torch.tensor(inputs.to_numpy(dtype=float))
y = torch.tensor(outputs.to_numpy(dtype=float))
X, y
```  
### 1.7 线性代数  
#### 1.7.1 标量  
标量由只有一个元素的张量表示  
#### 1.7.2 向量  
向量可以被视为标量值组成的列表  
人们通过一维张量表示向量  
```python
x = torch.arange(4)
x
```  
```python  
tensor([0, 1, 2, 3])
```  
可以使用下标来引用向量的任一元素，例如可以通过xi来引用第i个元素  

向量的长度通常称为向量的维度（dimension）  
与普通的Python数组一样，我们可以通过调用Python的内置len()函数来访问张量的长度  
```python
len(x)
```  
当用张量表示一个向量（只有一个轴）时，我们也可以通过.shape属性访问向量的长度  
```python
x.shape
```  
**注意** ：维度（dimension）这个词在不同上下文时往往会有不同的含义  
* 向量或轴的维度被用来表示向量或轴的长度，即向量或轴的元素数量  
* 张量的维度用来表示张量具有的轴数  
#### 1.7.3 矩阵  
正如向量将标量从零阶推广到一阶，矩阵将向量从一阶推广到二阶  
当调用函数来实例化张量时，我们可以通过指定两个分量m和n来创建一个形状为m × n的矩阵  
```python
A = torch.arange(20).reshape(5, 4)
A
```  
现在在代码中访问矩阵的转置  
```python
A.T  
```  
#### 1.7.4 张量  
就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构  
张量是描述具有任意数量轴的n维数组的通用方法  

开始处理图像时，张量将变得更加重要，图像以n维数组形式出现，其中3个轴对应于高度、宽度，以及一个通道（channel）轴，用于表示颜色通道（红色、绿色和蓝色）  
#### 1.7.5 张量算法的基本形质  
将两个相同形状的矩阵相加，
会在这两个矩阵上执行元素加法  
```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone() # 通过分配新内存，将A的一个副本分配给B
A, A + B
```  
```python
(tensor([[ 0., 1., 2., 3.],
[ 4., 5., 6., 7.],
[ 8., 9., 10., 11.],
[12., 13., 14., 15.],
[16., 17., 18., 19.]]),
tensor([[ 0., 2., 4., 6.],
[ 8., 10., 12., 14.],
[16., 18., 20., 22.],
[24., 26., 28., 30.],
[32., 34., 36., 38.]]))
```  
两个矩阵的按元素乘法称为Hadamard积（Hadamard product）（数学符号⊙）  
```python
A * B 
```  
```python
tensor([[ 0., 1., 4., 9.],
[ 16., 25., 36., 49.],
[ 64., 81., 100., 121.],
[144., 169., 196., 225.],
[256., 289., 324., 361.]])
```  
* 将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘  
#### 1.7.6 降维  
对任意张量进行的一个有用的操作是计算其元素的和  
```python  
x = torch.arange(4, dtype=torch.float32)
x, x.sum()
```  
可以表示任意形状张量的元素和  
```python
A.shape, A.sum()
```  
默认情况下，调用求和函数会沿所有的轴降低张量的维度，使它变为一个标量  
还可以指定张量沿哪一个轴来通过求和降低维度  

以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指
定axis=0  
由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失  
```python
A_sum_axis0 = A.sum(axis=0)
A_sum_axis0, A_sum_axis0.shape
```  
指定axis=1将通过汇总所有列的元素降维（轴1）  
因此，输入轴1的维数在输出形状中消失  
```python
A_sum_axis1 = A.sum(axis=1)
A_sum_axis1, A_sum_axis1.shape
```  
* 沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和  
```python
A.sum(axis=[0, 1])
```  
*平均值*  
调用函数来计算任意形状张量的平均值  
```python
A.mean(), A.sum() / A.numel()
```  
计算平均值的函数也可以沿指定轴降低张量的维度  
```python
A.mean(axis=0), A.sum(axis=0) / A.shape[0]
```  
* **非降维求和**  
有时在调用函数来计算总和或均值时保持轴数不变会很有用  
```python
sum_A = A.sum(axis=1, keepdims=True)
sum_A
```  
```
tensor([[ 6.],
        [22.],
        [38.],
        [54.],
        [70.]])
```  
由于sum_A在对每行进行求和后仍保持两个轴，可以通过广播将A除以sum_A  
```python
A / sum_A
```  
想沿某个轴计算A元素的累积总和，比如axis=0（按行计算），可以调用cumsum函数  
此函数不会沿任何轴降低输入张量的维度  
```python
A.cumsum(axis=0)
```  
#### 1.7.7 点积  
```python
torch.ones(4, dtype = torch.float32)
x, y, torch.dot(x, y)
```  
可以通过执行按元素乘法，然后进行求和来表示两个向量的点积 ：  
```python
torch.sum(x * y)
```  
#### 1.7.8 矩阵-向量积  
在代码中使用张量表示矩阵‐向量积，使用mv函数  
当为矩阵A和向量x调用torch.mv(A, x)时，会执行矩阵‐向量积  

**注意** ： A的列维数（沿轴1的长度）必须与x的维数（其长度）相同  
```python  
A.shape, x.shape, torch.mv(A, x)
```  
#### 1.7.9 矩阵-矩阵乘法  
可以将矩阵‐矩阵乘法AB看作简单地执行m次矩阵‐向量积，并将结果拼接在一起，形成一个n × m矩阵  
```python
B = torch.ones(4, 3)
torch.mm(A, B)
```  
#### 1.7.10 范数  
非正式地说，向量的范数是表示一个向量有多大  
这里考虑的大小概念不涉及维度，而是分量的大小  

在线性代数中，向量范数是将向量映射到标量的函数f  
给定任意向量x，向量范数要满足一些属性  

* 第一个性质是：如果按常数因子α缩放向量的所有元素，其范数也会按相同常数因子的绝对值缩放 ： 
```math 
f(αx) = |α|f(x)
```  
* 第二个性质是熟悉的三角不等式 :  
```math  
f(x + y) ≤ f(x) + f(y)  
```  
* 第三个性质简单地说范数必须是非负的 :  
```math
f(x) ≥ 0
```  

事实上，欧几里得距离是一个L2范数：假设n维向量x中的元素是x1, . . . , xn，其L2范数是向量元素平方和的平方根  
在代码中，我们可以按如下方式计算向量的L2范数
```python  
u = torch.tensor([3.0, -4.0])
torch.norm(u)
```  
深度学习中更经常地使用L2范数的平方，也会经常遇到L1范数，它表示为向量元素的绝对值之和 ：  
```python
torch.abs(u).sum()
```  
### 1.8 微积分  
#### 1.8.1 导数与微分  
例子 ：定义u = f(x) 如下 ：  
```python  
%matplotlib inline
import numpy as np
from matplotlib_inline import backend_inline
from d2l import torch as d2l
def f(x):
return 3 * x ** 2 - 4 * x  
```  
```python
def numerical_lim(f, x, h):
return (f(x + h) - f(x)) / h
h = 0.1
for i in range(5):
print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')
h *= 0.1
```  
可得输出结果为 ：  
```python
h=0.10000, numerical limit=2.30000
h=0.01000, numerical limit=2.03000
h=0.00100, numerical limit=2.00300
h=0.00010, numerical limit=2.00030
h=0.00001, numerical limit=2.00003
```  
#### 1.8.2 偏导数  
#### 1.8.3 梯度  
函数f(x)相对于x的梯度是一个包含n个偏导数的向量  
#### 1.8.4 链式法则  
根据链式法则 ：  
```math
dy/dx =dy/du*du/dx
```  
### 1.9 自动微分  
```python
x.requires_grad_(True) # 等价于x=torch.arange(4.0,requires_grad=True)
x.grad # 默认值是None
```  
现在计算 y 值  
```python
y = 2 * torch.dot(x, x)
y
```  
x是一个长度为4的向量，计算x和x的点积，得到了我们赋值给y的标量输出  
接下来，通过调用反向传播函数来自动计算y关于x每个分量的梯度，并打印这些梯度  
```python
y.backward()
x.grad
```  
#### 1.9.1 非标量变量的反向传播  
当y不是标量时，向量y关于向量x的导数的最自然解释是一个矩阵  
对于高阶和高维的y和x，求导的结果可以是一个高阶张量  

但当调用向量的反向计算时，通常会试图计算一批训练样本中每个组成部分的损失函数的导数  
目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和  
```python
# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。
# 本例只想求偏导数的和，所以传递一个1的梯度是合适的
x.grad.zero_()
y = x * x
# 等价于y.backward(torch.ones(len(x)))
y.sum().backward()
x.grad
```  
#### 1.9.2 分离计算  
如果希望将某些计算移动到记录的计算图之外  
想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数，并且只考虑到x在y被计算后发挥的作用  

这里可以分离y来返回一个新变量u，该变量与y具有相同的值，但丢弃计算图中计算y任何信息  
因此，下面的反向传播函数计算z=u * x关于x的偏导数，同时将u作为常数处理，而不是z=x * x * x关于x的偏导数  
```python
x.grad.zero_()
y = x * x
u = y.detach()
z = u * x
z.sum().backward()
x.grad == u
```  
由于记录了y的计算结果，我们可以随后在y上调用反向传播，得到y=x * x关于的x的导数，即2 * x  
```python
x.grad.zero_()
y.sum().backward()
x.grad == 2 * x
```  
#### 1.9.3 Python控制流的梯度计算  
使用自动微分的一个好处是：即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度  
```python
def f(a):
b = a * 2
while b.norm() < 1000:
b = b * 2
if b.sum() > 0:
c = b
else:
c = 100 * b
return c
```  
计算梯度 ：  
```python  
a = torch.randn(size=(), requires_grad=True)
d = f(a)
d.backward()
```  

## 2. 线性神经网络  
### 2.1 线性回归  
回归（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法  
#### 2.1.1 线性回归的基本元素  
线性回归基于几个简单的假设：首先，假设自变量 **x** 和因变量 y 之间的关系是线性的，即 y 可以表示为 **x** 中元素的加权和，这里通常允许包含观测值的一些噪声；其次，我们假设任何噪声都比较正常，如噪声遵循正态分布  

**线性假设**  
以一个房屋的案例 ：  
线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和，如下面的式子 ：  
```math
price = warea · area + wage · age + b.  
```  
warea和wage 称为权重（weight），权重决定了每个特征对我们预测值的影响  
b称为偏置（bias）、偏移量（offset）或截距（intercept）  
偏置是指当所有特征都取值为0时，预测值应该为多少  

给定一个数据集，我们的目标是寻找模型的权重 **w** 和偏置 b ，使得根据模型做出的预测大体符合数据里的真实价格  

而在机器学习领域，通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。输入包含d个特征时，我们将预测结果yˆ （通常使用“尖角”符号表示y的估计值）表示为 :  
```math
yˆ = w1x1 + ... + wdxd + b.  
```  
将所有特征放到向量**x** ∈ Rd中，并将所有权重放到向量**w** ∈ Rd中，我们可以用点积形式来简洁地表达模型 ：  
```math
yˆ = w⊤x + b.
```  
**损失函数**  
开始考虑如何用模型拟合（fit）数据之前，需要确定一个拟合程度的度量  
损失函数（loss function）能够量化目标的实际值与预测值之间的差距  

通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0  
回归问题中最常用的损失函数是平方误差函数  
当样本i的预测值为yˆ(i)，其相应的真实标签为y(i)时，平方误差可以定义为以下公式 ：  
$$
l^i(w,b)={(\hat y^i -y^i)^2/2}
$$  
在训练模型时，我们希望寻找一组参数（w∗, b∗），这组参数能最小化在所有训练样本上的总损失  

**解析解**  
线性回归的解可以用一个公式简单地表达出来，这类解叫作解析解（analytical solution）  

***随机梯度下降***  
即便在无法得到解析解，仍然可以有效地训练模型  
用一种名为梯度下降（gradient descent）的方法，这种方法几乎可以优化所有深度学习模型  
梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值）关于模型参数的导数（在这里也可以称为梯度），但是这种方式在每一次更新参数时会遍历一遍数据库
因此，通常会在每次需要计算更新的时候随机抽取一小批样本，这种变体叫做小批量随机梯度下降  

算法的步骤如下：（1）初始化模型参数的值，如随机初始化；（2）从数据集中随机抽取小批量样
本且在负梯度的方向上更新参数，并不断迭代这一步骤  

在训练了预先确定的若干迭代次数后（或者直到满足某些其他停止条件后），记录下模型参数的估计值，表示为
$$\hat w ,\hat b$$  
但是即使函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值  
因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值  
### 2.2 softmax 运算  
#### 2.2.1 网络架构  
为了估计所有可能类别的条件概率，需要一个有多个输出的模型，每个类别对应一个输出  
为了解决线性模型的分类问题，需要和输出一样多的仿射函数  

例子中，由于有4个特征和3个可能的输出类别   
将需要12个标量来表示权重  
3个标量来表示偏置 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true" columnalign="right" columnspacing="0em" rowspacing="3pt">
    <mtr>
      <mtd>
        <mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
          <mtr>
            <mtd>
              <msub>
                <mi>o</mi>
                <mn>1</mn>
              </msub>
            </mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <msub>
                <mi>x</mi>
                <mn>1</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>11</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>2</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>12</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>3</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>13</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>4</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>14</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>b</mi>
                <mn>1</mn>
              </msub>
              <mo>,</mo>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <msub>
                <mi>o</mi>
                <mn>2</mn>
              </msub>
            </mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <msub>
                <mi>x</mi>
                <mn>1</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>21</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>2</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>22</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>3</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>23</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>4</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>24</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>b</mi>
                <mn>2</mn>
              </msub>
              <mo>,</mo>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <msub>
                <mi>o</mi>
                <mn>3</mn>
              </msub>
            </mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <msub>
                <mi>x</mi>
                <mn>1</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>31</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>2</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>32</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>3</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>33</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>x</mi>
                <mn>4</mn>
              </msub>
              <msub>
                <mi>w</mi>
                <mrow data-mjx-texclass="ORD">
                  <mn>34</mn>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>b</mi>
                <mn>3</mn>
              </msub>
              <mo>.</mo>
            </mtd>
          </mtr>
        </mtable>
      </mtd>
    </mtr>
  </mtable>
</math>  
可以用神经网络图 图3.4.1来描述这个计算过程  
与线性回归一样，softmax回归也是一个单层神经网络 :  
![图像](https://zh.d2l.ai/_images/softmaxreg.svg)  
通过向量形式表达为<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">o</mi>
  </mrow>
  <mo>=</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">W</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo>+</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">b</mi>
  </mrow>
</math>这是一种更适合数学和编写代码的形式  
已经将所有权重放到一个3*4矩阵中  

#### 2.2.2 softmax 运算  
为了得到预测结果，将设置一个阈值，如选择具有最大概率的标签  
要将输出视为概率，必须保证在任何数据上的输出都是非负的且总和为1  
需要一个训练的目标函数，来激励模型精准地估计概率  

softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持可导的性质  
为了完成这一目标，首先对每个未规范化的预测求幂，这样可以确保输出非负  
为了确保最终输出的概率值总和为1，再让每个求幂后的结果除以它们的总和，如下式 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow data-mjx-texclass="ORD">
    <mover>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">y</mi>
      </mrow>
      <mo stretchy="false">^</mo>
    </mover>
  </mrow>
  <mo>=</mo>
  <mrow data-mjx-texclass="ORD">
    <mi data-mjx-auto-op="false">softmax</mi>
  </mrow>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">o</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mstyle scriptlevel="0">
    <mspace width="1em"></mspace>
  </mstyle>
  <mtext>&#x5176;&#x4E2D;</mtext>
  <mstyle scriptlevel="0">
    <mspace width="1em"></mspace>
  </mstyle>
  <msub>
    <mrow data-mjx-texclass="ORD">
      <mover>
        <mi>y</mi>
        <mo stretchy="false">^</mo>
      </mover>
    </mrow>
    <mi>j</mi>
  </msub>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>o</mi>
        <mi>j</mi>
      </msub>
      <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
      <munder>
        <mo data-mjx-texclass="OP">&#x2211;</mo>
        <mi>k</mi>
      </munder>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>o</mi>
        <mi>k</mi>
      </msub>
      <mo stretchy="false">)</mo>
    </mrow>
  </mfrac>
</math>  

#### 2.2.3 损失函数  
需要一个损失函数来度量预测的效果  
将使用最大似然估计  
##### 2.2.3.1 对数似然  
softmax函数给出了一个向量 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mover>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">y</mi>
      </mrow>
      <mo stretchy="false">^</mo>
    </mover>
  </mrow>
</math> 可以将其视为“对给定任意输入 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
</math> 的每个类的条件概率”  

可以将估计值与实际值进行比较 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>P</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">Y</mi>
  </mrow>
  <mo>&#x2223;</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">X</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <munderover>
    <mo data-mjx-texclass="OP">&#x220F;</mo>
    <mrow data-mjx-texclass="ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>n</mi>
  </munderover>
  <mi>P</mi>
  <mo stretchy="false">(</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">y</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>&#x2223;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">x</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo stretchy="false">)</mo>
  <mo>.</mo>
</math>   
根据最大似然估计，最大化 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>P</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">Y</mi>
  </mrow>
  <mo>&#x2223;</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">X</mi>
  </mrow>
  <mo stretchy="false">)</mo>
</math> 相当于最小化负对数似然 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mo>&#x2212;</mo>
  <mi>log</mi>
  <mo data-mjx-texclass="NONE">&#x2061;</mo>
  <mi>P</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">Y</mi>
  </mrow>
  <mo>&#x2223;</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">X</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <munderover>
    <mo data-mjx-texclass="OP">&#x2211;</mo>
    <mrow data-mjx-texclass="ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>n</mi>
  </munderover>
  <mo>&#x2212;</mo>
  <mi>log</mi>
  <mo data-mjx-texclass="NONE">&#x2061;</mo>
  <mi>P</mi>
  <mo stretchy="false">(</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">y</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>&#x2223;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">x</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <munderover>
    <mo data-mjx-texclass="OP">&#x2211;</mo>
    <mrow data-mjx-texclass="ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>n</mi>
  </munderover>
  <mi>l</mi>
  <mo stretchy="false">(</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">y</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>,</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mover>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">y</mi>
        </mrow>
        <mo stretchy="false">^</mo>
      </mover>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mi>i</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo stretchy="false">)</mo>
  <mo>,</mo>
</math>  
其中，对于任何标签 **y** 和预测模型 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mover>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">y</mi>
      </mrow>
      <mo stretchy="false">^</mo>
    </mover>
  </mrow>
</math> 损失函数为 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>l</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">y</mi>
  </mrow>
  <mo>,</mo>
  <mrow data-mjx-texclass="ORD">
    <mover>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">y</mi>
      </mrow>
      <mo stretchy="false">^</mo>
    </mover>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mo>&#x2212;</mo>
  <munderover>
    <mo data-mjx-texclass="OP">&#x2211;</mo>
    <mrow data-mjx-texclass="ORD">
      <mi>j</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>q</mi>
  </munderover>
  <msub>
    <mi>y</mi>
    <mi>j</mi>
  </msub>
  <mi>log</mi>
  <mo data-mjx-texclass="NONE">&#x2061;</mo>
  <msub>
    <mrow data-mjx-texclass="ORD">
      <mover>
        <mi>y</mi>
        <mo stretchy="false">^</mo>
      </mover>
    </mrow>
    <mi>j</mi>
  </msub>
  <mo>.</mo>
</math>  
该类损失函数通常被称为交叉熵损失函数  
##### 2.2.3.2 softmax 及其导数  
利用softmax的定义，我们得到 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true" columnalign="right" columnspacing="0em" rowspacing="3pt">
    <mtr>
      <mtd>
        <mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
          <mtr>
            <mtd>
              <mi>l</mi>
              <mo stretchy="false">(</mo>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">y</mi>
              </mrow>
              <mo>,</mo>
              <mrow data-mjx-texclass="ORD">
                <mover>
                  <mrow data-mjx-texclass="ORD">
                    <mi mathvariant="bold">y</mi>
                  </mrow>
                  <mo stretchy="false">^</mo>
                </mover>
              </mrow>
              <mo stretchy="false">)</mo>
            </mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <mo>&#x2212;</mo>
              <munderover>
                <mo data-mjx-texclass="OP">&#x2211;</mo>
                <mrow data-mjx-texclass="ORD">
                  <mi>j</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>q</mi>
              </munderover>
              <msub>
                <mi>y</mi>
                <mi>j</mi>
              </msub>
              <mi>log</mi>
              <mo data-mjx-texclass="NONE">&#x2061;</mo>
              <mfrac>
                <mrow>
                  <mi>exp</mi>
                  <mo data-mjx-texclass="NONE">&#x2061;</mo>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mi>o</mi>
                    <mi>j</mi>
                  </msub>
                  <mo stretchy="false">)</mo>
                </mrow>
                <mrow>
                  <munderover>
                    <mo data-mjx-texclass="OP">&#x2211;</mo>
                    <mrow data-mjx-texclass="ORD">
                      <mi>k</mi>
                      <mo>=</mo>
                      <mn>1</mn>
                    </mrow>
                    <mi>q</mi>
                  </munderover>
                  <mi>exp</mi>
                  <mo data-mjx-texclass="NONE">&#x2061;</mo>
                  <mo stretchy="false">(</mo>
                  <msub>
                    <mi>o</mi>
                    <mi>k</mi>
                  </msub>
                  <mo stretchy="false">)</mo>
                </mrow>
              </mfrac>
            </mtd>
          </mtr>
          <mtr>
            <mtd></mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <munderover>
                <mo data-mjx-texclass="OP">&#x2211;</mo>
                <mrow data-mjx-texclass="ORD">
                  <mi>j</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>q</mi>
              </munderover>
              <msub>
                <mi>y</mi>
                <mi>j</mi>
              </msub>
              <mi>log</mi>
              <mo data-mjx-texclass="NONE">&#x2061;</mo>
              <munderover>
                <mo data-mjx-texclass="OP">&#x2211;</mo>
                <mrow data-mjx-texclass="ORD">
                  <mi>k</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>q</mi>
              </munderover>
              <mi>exp</mi>
              <mo data-mjx-texclass="NONE">&#x2061;</mo>
              <mo stretchy="false">(</mo>
              <msub>
                <mi>o</mi>
                <mi>k</mi>
              </msub>
              <mo stretchy="false">)</mo>
              <mo>&#x2212;</mo>
              <munderover>
                <mo data-mjx-texclass="OP">&#x2211;</mo>
                <mrow data-mjx-texclass="ORD">
                  <mi>j</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>q</mi>
              </munderover>
              <msub>
                <mi>y</mi>
                <mi>j</mi>
              </msub>
              <msub>
                <mi>o</mi>
                <mi>j</mi>
              </msub>
            </mtd>
          </mtr>
          <mtr>
            <mtd></mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <mi>log</mi>
              <mo data-mjx-texclass="NONE">&#x2061;</mo>
              <munderover>
                <mo data-mjx-texclass="OP">&#x2211;</mo>
                <mrow data-mjx-texclass="ORD">
                  <mi>k</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>q</mi>
              </munderover>
              <mi>exp</mi>
              <mo data-mjx-texclass="NONE">&#x2061;</mo>
              <mo stretchy="false">(</mo>
              <msub>
                <mi>o</mi>
                <mi>k</mi>
              </msub>
              <mo stretchy="false">)</mo>
              <mo>&#x2212;</mo>
              <munderover>
                <mo data-mjx-texclass="OP">&#x2211;</mo>
                <mrow data-mjx-texclass="ORD">
                  <mi>j</mi>
                  <mo>=</mo>
                  <mn>1</mn>
                </mrow>
                <mi>q</mi>
              </munderover>
              <msub>
                <mi>y</mi>
                <mi>j</mi>
              </msub>
              <msub>
                <mi>o</mi>
                <mi>j</mi>
              </msub>
              <mo>.</mo>
            </mtd>
          </mtr>
        </mtable>
      </mtd>
    </mtr>
  </mtable>
</math>  
考虑相对于任何未规范化的预测 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msub>
    <mi>o</mi>
    <mi>j</mi>
  </msub>
</math> 的导数，我们得到 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <msub>
    <mi>&#x2202;</mi>
    <mrow data-mjx-texclass="ORD">
      <msub>
        <mi>o</mi>
        <mi>j</mi>
      </msub>
    </mrow>
  </msub>
  <mi>l</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">y</mi>
  </mrow>
  <mo>,</mo>
  <mrow data-mjx-texclass="ORD">
    <mover>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">y</mi>
      </mrow>
      <mo stretchy="false">^</mo>
    </mover>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>o</mi>
        <mi>j</mi>
      </msub>
      <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
      <munderover>
        <mo data-mjx-texclass="OP">&#x2211;</mo>
        <mrow data-mjx-texclass="ORD">
          <mi>k</mi>
          <mo>=</mo>
          <mn>1</mn>
        </mrow>
        <mi>q</mi>
      </munderover>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <msub>
        <mi>o</mi>
        <mi>k</mi>
      </msub>
      <mo stretchy="false">)</mo>
    </mrow>
  </mfrac>
  <mo>&#x2212;</mo>
  <msub>
    <mi>y</mi>
    <mi>j</mi>
  </msub>
  <mo>=</mo>
  <mrow data-mjx-texclass="ORD">
    <mi data-mjx-auto-op="false">softmax</mi>
  </mrow>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">o</mi>
  </mrow>
  <msub>
    <mo stretchy="false">)</mo>
    <mi>j</mi>
  </msub>
  <mo>&#x2212;</mo>
  <msub>
    <mi>y</mi>
    <mi>j</mi>
  </msub>
  <mo>.</mo>
</math>  

##### 2.2.3.3 交叉熵损失  
现在用一个概率向量表示，如 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mo stretchy="false">(</mo>
  <mn>0.1</mn>
  <mo>,</mo>
  <mn>0.2</mn>
  <mo>,</mo>
  <mn>0.7</mn>
  <mo stretchy="false">)</mo>
</math> ，而不是仅包含二元项 (0,0,1) 的向量  
定义一个损失 *l* ,它是所有标签分布的预期损失值  
此损失被称为**交叉熵损失**，它是分类问题最常用的损失之一  

## 3. 多层感知机  
### 3.1 多层感知机  
#### 3.1.1 隐藏层  
在之前章节中描述了仿射变换，它是一种带偏置的线性变化  
该模型通过单个仿射变换将我们的输入直接映射到输出，然后进行softmax操作  
如果标签通过仿射变换后确实与输入数据相关，那么这种方法确实足够了  
但是仿射中的线性是一种很强的假设  
##### 3.1.1.1 线性变换的局限性  
线性意味着单调假设  
很容易找到违反单调性的假设 ：例如根据人的体温预测死亡率  
##### 3.1.1.2 在网络中加入隐藏层  
可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型  
要做到这一点，最简单的方法是将许多全连接层堆叠在一起  
这种架构通常称为多层感知机，通常缩写为MLP  
![图片](https://zh.d2l.ai/_images/mlp.svg)  
注意，这两个层都是全连接的  
每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元  
##### 3.1.1.3 从线性到非线性  
同之前的章节一样， 我们通过矩阵<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">X</mi>
  </mrow>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>n</mi>
      <mo>&#xD7;</mo>
      <mi>d</mi>
    </mrow>
  </msup>
</math>来表示 *n* 个样本的小批量，其中每个样本具有个输入 *d* 特征  
对于具有 *h* 个隐藏单元的单隐藏层多层感知机，用<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">H</mi>
  </mrow>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>n</mi>
      <mo>&#xD7;</mo>
      <mi>h</mi>
    </mrow>
  </msup>
</math>表示隐藏层的输出，称为隐藏表示  
因为隐藏层和输出层都是全连接的， 所以我们有隐藏层权重<math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>d</mi>
      <mo>&#xD7;</mo>
      <mi>h</mi>
    </mrow>
  </msup>
</math> 和隐藏层偏置<math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">b</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mn>1</mn>
      <mo>&#xD7;</mo>
      <mi>h</mi>
    </mrow>
  </msup>
</math> 以及输出层权重<math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>h</mi>
      <mo>&#xD7;</mo>
      <mi>q</mi>
    </mrow>
  </msup>
</math> 和输出层偏置<math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">b</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mn>1</mn>
      <mo>&#xD7;</mo>
      <mi>q</mi>
    </mrow>
  </msup>
</math>  
按如下方式计算单隐藏层多层感知机的输出 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">O</mi>
  </mrow>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>n</mi>
      <mo>&#xD7;</mo>
      <mi>q</mi>
    </mrow>
  </msup>
</math>  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true" columnalign="right" columnspacing="0em" rowspacing="3pt">
    <mtr>
      <mtd>
        <mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
          <mtr>
            <mtd>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">H</mi>
              </mrow>
            </mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">X</mi>
              </mrow>
              <msup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="bold">W</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">(</mo>
                  <mn>1</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="bold">b</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">(</mo>
                  <mn>1</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>,</mo>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">O</mi>
              </mrow>
            </mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">H</mi>
              </mrow>
              <msup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="bold">W</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">(</mo>
                  <mn>2</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="bold">b</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">(</mo>
                  <mn>2</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>.</mo>
            </mtd>
          </mtr>
        </mtable>
      </mtd>
    </mtr>
  </mtable>
</math>  
合并隐藏层，便可产生具有参数<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">W</mi>
  </mrow>
  <mo>=</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
</math> 和<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">b</mi>
  </mrow>
  <mo>=</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">b</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>+</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">b</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
</math> 的等价单层模型 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">O</mi>
  </mrow>
  <mo>=</mo>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">X</mi>
  </mrow>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>+</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">b</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo stretchy="false">)</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>+</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">b</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>=</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">X</mi>
  </mrow>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>+</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">b</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>+</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">b</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>=</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">X</mi>
  </mrow>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">W</mi>
  </mrow>
  <mo>+</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">b</mi>
  </mrow>
  <mo>.</mo>
</math>  
为了发挥多层架构的潜力，还需要一个额外的关键要素：在仿射变换之后对每个隐藏单元应用非线性的激活函数 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x3C3;</mi>
</math> 
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true" columnalign="right" columnspacing="0em" rowspacing="3pt">
    <mtr>
      <mtd>
        <mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
          <mtr>
            <mtd>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">H</mi>
              </mrow>
            </mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <mi>&#x3C3;</mi>
              <mo stretchy="false">(</mo>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">X</mi>
              </mrow>
              <msup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="bold">W</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">(</mo>
                  <mn>1</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="bold">b</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">(</mo>
                  <mn>1</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo stretchy="false">)</mo>
              <mo>,</mo>
            </mtd>
          </mtr>
          <mtr>
            <mtd>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">O</mi>
              </mrow>
            </mtd>
            <mtd>
              <mi></mi>
              <mo>=</mo>
              <mrow data-mjx-texclass="ORD">
                <mi mathvariant="bold">H</mi>
              </mrow>
              <msup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="bold">W</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">(</mo>
                  <mn>2</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>+</mo>
              <msup>
                <mrow data-mjx-texclass="ORD">
                  <mi mathvariant="bold">b</mi>
                </mrow>
                <mrow data-mjx-texclass="ORD">
                  <mo stretchy="false">(</mo>
                  <mn>2</mn>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msup>
              <mo>.</mo>
            </mtd>
          </mtr>
        </mtable>
      </mtd>
    </mtr>
  </mtable>
</math>  

#### 3.1.2 激活函数  
*激活函数* 通过计算加权和并加上偏置来确定神经元是否应该被激活，它们将输入信号转换为输出的可微运算  
##### 3.1.2.1 ReLU 函数  
最受欢迎的激活函数是修正线性单元  
因为它实现简单，同时在各种预测任务中表现良好  
给定元素 **x** ，ReLU函数被定义为该元素与的最大值 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>ReLU</mi>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mo data-mjx-texclass="OP" movablelimits="true">max</mo>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo>,</mo>
  <mn>0</mn>
  <mo stretchy="false">)</mo>
  <mo>.</mo>
</math>  
可见图像为 ：  

![图片](https://zh.d2l.ai/_images/output_mlp_76f463_18_1.svg)  
ReLU函数的导数图像 ：  

![图像](https://zh.d2l.ai/_images/output_mlp_76f463_33_1.svg)  
使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过  
并且ReLU减轻了困扰以往神经网络的梯度消失问题  
ReLU函数有许多变体，包括参数化ReLU函数   
该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>pReLU</mi>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mo data-mjx-texclass="OP" movablelimits="true">max</mo>
  <mo stretchy="false">(</mo>
  <mn>0</mn>
  <mo>,</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>+</mo>
  <mi>&#x3B1;</mi>
  <mo data-mjx-texclass="OP" movablelimits="true">min</mo>
  <mo stretchy="false">(</mo>
  <mn>0</mn>
  <mo>,</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>.</mo>
</math>  

##### 3.1.2.2 sigmoid 函数   
对于一个定义域在 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="double-struck">R</mi>
  </mrow>
</math> 中的输入， sigmoid函数将输入变换为区间(0, 1)上的输出  
sigmoid通常称为挤压函数  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>sigmoid</mi>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mfrac>
    <mn>1</mn>
    <mrow>
      <mn>1</mn>
      <mo>+</mo>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <mo>&#x2212;</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </mfrac>
  <mo>.</mo>
</math>  

想要将输出视作二元分类问题的概率时， sigmoid仍然被广泛用作输出单元上的激活函数  
该函数图像为 ：  

![图片](https://zh.d2l.ai/_images/output_mlp_76f463_48_0.svg)  
igmoid函数的导数为下面的公式 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mfrac>
    <mi>d</mi>
    <mrow>
      <mi>d</mi>
      <mi>x</mi>
    </mrow>
  </mfrac>
  <mi>sigmoid</mi>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <mo>&#x2212;</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo>+</mo>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <mo>&#x2212;</mo>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
      <msup>
        <mo stretchy="false">)</mo>
        <mn>2</mn>
      </msup>
    </mrow>
  </mfrac>
  <mo>=</mo>
  <mi>sigmoid</mi>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">(</mo>
    <mn>1</mn>
    <mo>&#x2212;</mo>
    <mi>sigmoid</mi>
    <mo stretchy="false">(</mo>
    <mi>x</mi>
    <mo stretchy="false">)</mo>
    <mo data-mjx-texclass="CLOSE">)</mo>
  </mrow>
  <mo>.</mo>
</math>  

sigmoid函数的导数图像如下所示 ：  

![图片](https://zh.d2l.ai/_images/output_mlp_76f463_63_0.svg)  
这是会发现，当函数越是远离 0 点时，导数越接近 0 ， 该现象即使“梯度消失现象”  
##### 3.1.2.3 tanh 函数  
与sigmoid函数类似， tanh(双曲正切)函数也能将其输入压缩转换到区间(-1, 1)上

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>tanh</mi>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mn>1</mn>
      <mo>&#x2212;</mo>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <mo>&#x2212;</mo>
      <mn>2</mn>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
      <mn>1</mn>
      <mo>+</mo>
      <mi>exp</mi>
      <mo data-mjx-texclass="NONE">&#x2061;</mo>
      <mo stretchy="false">(</mo>
      <mo>&#x2212;</mo>
      <mn>2</mn>
      <mi>x</mi>
      <mo stretchy="false">)</mo>
    </mrow>
  </mfrac>
  <mo>.</mo>
</math>  
图像如下 ： 

![图片](https://zh.d2l.ai/_images/output_mlp_76f463_78_0.svg)  

tanh 的导数为  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mfrac>
    <mi>d</mi>
    <mrow>
      <mi>d</mi>
      <mi>x</mi>
    </mrow>
  </mfrac>
  <mi>tanh</mi>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mn>1</mn>
  <mo>&#x2212;</mo>
  <msup>
    <mi>tanh</mi>
    <mn>2</mn>
  </msup>
  <mo stretchy="false">(</mo>
  <mi>x</mi>
  <mo stretchy="false">)</mo>
  <mo>.</mo>
</math>  

tanh函数的导数图像如下所示，与sigmoid 函数类似  

![图片](https://zh.d2l.ai/_images/output_mlp_76f463_93_0.svg)  

### 3.2 模型选择、欠拟合与过拟合  
#### 3.2.1 训练误差与泛化误差  
*训练误差* 是指， 模型在训练数据集上计算得到的误差  
*泛化误差* 是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望  
#### 3.2.2 模型选择  
通常在评估几个候选模型后选择最终的模型  
这个过程叫做模型选择  
##### 3.2.2.1 验证集  
常见的作法是将数据分成三份  
除了训练与测试以外  
还需要加一个验证数据集  
##### 3.2.2.2 K 折交叉验证  
当训练数据稀缺时，可能无法提供足够的数据来构成一个合适的验证集  
这个问题的一个流行的解决方案是采用 *K* 折交叉验证  

原始训练数据被分成 *K* 个不重叠的子集  
然后执行 *K* 次模型训练与验证  
每次在 *K - 1* 个子集上进行训练 ，在剩下那个子集上验证  
最后，对 *K* 次实验的结果取平均来训练和验证误差  
#### 3.2.3 欠拟合与过拟合  
模型过于简单，表达能力不足  
训练和验证误差之间的泛化误差很小 ，称为 *欠拟合*  

另一方面 ，训练误差明显低于验证误差 ，成为 *过拟合*  
##### 3.2.3.1 模型复杂度  
给定一个多项式的例子  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow data-mjx-texclass="ORD">
    <mover>
      <mi>y</mi>
      <mo stretchy="false">^</mo>
    </mover>
  </mrow>
  <mo>=</mo>
  <munderover>
    <mo data-mjx-texclass="OP">&#x2211;</mo>
    <mrow data-mjx-texclass="ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>0</mn>
    </mrow>
    <mi>d</mi>
  </munderover>
  <msup>
    <mi>x</mi>
    <mi>i</mi>
  </msup>
  <msub>
    <mi>w</mi>
    <mi>i</mi>
  </msub>
</math>  

这只是一个线性回归问题，我们的特征是 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>x</mi>
</math> 的幂给出的，模型的权重是 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msub>
    <mi>w</mi>
    <mi>i</mi>
  </msub>
</math> 给出的，偏置是 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msub>
    <mi>w</mi>
    <mn>0</mn>
  </msub>
</math>给出的  

下图直观给出多项式的阶数和欠拟合与过拟合之间的关系  

![图片](https://zh.d2l.ai/_images/capacity-vs-error.svg)  
### 3.3 权重衰减  
在训练参数化机器学习模型时，*权重衰减* 是最广泛使用的正则化的技术之一， 它通常也被称为 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msub>
    <mi>L</mi>
    <mn>2</mn>
  </msub>
</math> 正则化  

一种简单的方法是通过线性函数 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>f</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">w</mi>
    </mrow>
    <mi mathvariant="normal">&#x22A4;</mi>
  </msup>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
</math> 中的权重向量的某个范数来度量其复杂性，例如 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mo data-mjx-texclass="ORD" fence="false" stretchy="false">&#x2016;</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">w</mi>
  </mrow>
  <msup>
    <mo data-mjx-texclass="ORD" fence="false" stretchy="false">&#x2016;</mo>
    <mn>2</mn>
  </msup>
</math>  
要保证权重向量比较小，最常用方法是将其范数作为惩罚项加到最小化损失的问题中  
如果权重向量增长太大，学习算法会集中最小化权重范数 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mo data-mjx-texclass="ORD" fence="false" stretchy="false">&#x2016;</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">w</mi>
  </mrow>
  <msup>
    <mo data-mjx-texclass="ORD" fence="false" stretchy="false">&#x2016;</mo>
    <mn>2</mn>
  </msup>
</math>  
损失由下式给出 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>L</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">w</mi>
  </mrow>
  <mo>,</mo>
  <mi>b</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mfrac>
    <mn>1</mn>
    <mi>n</mi>
  </mfrac>
  <munderover>
    <mo data-mjx-texclass="OP">&#x2211;</mo>
    <mrow data-mjx-texclass="ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mi>n</mi>
  </munderover>
  <mfrac>
    <mn>1</mn>
    <mn>2</mn>
  </mfrac>
  <msup>
    <mrow data-mjx-texclass="INNER">
      <mo data-mjx-texclass="OPEN">(</mo>
      <msup>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">w</mi>
        </mrow>
        <mi mathvariant="normal">&#x22A4;</mi>
      </msup>
      <msup>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mo stretchy="false">(</mo>
          <mi>i</mi>
          <mo stretchy="false">)</mo>
        </mrow>
      </msup>
      <mo>+</mo>
      <mi>b</mi>
      <mo>&#x2212;</mo>
      <msup>
        <mi>y</mi>
        <mrow data-mjx-texclass="ORD">
          <mo stretchy="false">(</mo>
          <mi>i</mi>
          <mo stretchy="false">)</mo>
        </mrow>
      </msup>
      <mo data-mjx-texclass="CLOSE">)</mo>
    </mrow>
    <mn>2</mn>
  </msup>
  <mo>.</mo>
</math>  
为了惩罚权重向量的大小，必须以某种方式在损失函数中添加 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mo data-mjx-texclass="ORD" fence="false" stretchy="false">&#x2016;</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">w</mi>
  </mrow>
  <msup>
    <mo data-mjx-texclass="ORD" fence="false" stretchy="false">&#x2016;</mo>
    <mn>2</mn>
  </msup>
</math>  

可以通过正则化常数 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x3BB;</mi>
</math>  来描述这种调整   
通过验证数据拟合 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>L</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">w</mi>
  </mrow>
  <mo>,</mo>
  <mi>b</mi>
  <mo stretchy="false">)</mo>
  <mo>+</mo>
  <mfrac>
    <mi>&#x3BB;</mi>
    <mn>2</mn>
  </mfrac>
  <mo data-mjx-texclass="ORD" fence="false" stretchy="false">&#x2016;</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">w</mi>
  </mrow>
  <msup>
    <mo data-mjx-texclass="ORD" fence="false" stretchy="false">&#x2016;</mo>
    <mn>2</mn>
  </msup>
  <mo>,</mo>
</math>  

* 对于 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x3BB;</mi>
  <mo>=</mo>
  <mn>0</mn>
</math>，我们恢复了原来的损失函数  
* 对于 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x3BB;</mi>
  <mo>&gt;</mo>
  <mn>0</mn>
</math>，我们限制 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mo data-mjx-texclass="ORD" fence="false" stretchy="false">&#x2016;</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">w</mi>
  </mrow>
  <mo data-mjx-texclass="ORD" fence="false" stretchy="false">&#x2016;</mo>
</math> 的大小  

<math xmlns="http://www.w3.org/1998/Math/MathML">
  <msub>
    <mi>L</mi>
    <mn>2</mn>
  </msub>
</math> 正则化回归的小批量随机梯度下降更新如下式 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true" columnalign="right left" columnspacing="0em" rowspacing="3pt">
    <mtr>
      <mtd>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">w</mi>
        </mrow>
      </mtd>
      <mtd>
        <mi></mi>
        <mo stretchy="false">&#x2190;</mo>
        <mrow data-mjx-texclass="INNER">
          <mo data-mjx-texclass="OPEN">(</mo>
          <mn>1</mn>
          <mo>&#x2212;</mo>
          <mi>&#x3B7;</mi>
          <mi>&#x3BB;</mi>
          <mo data-mjx-texclass="CLOSE">)</mo>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">w</mi>
        </mrow>
        <mo>&#x2212;</mo>
        <mfrac>
          <mi>&#x3B7;</mi>
          <mrow>
            <mo stretchy="false">|</mo>
            <mrow data-mjx-texclass="ORD">
              <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">B</mi>
            </mrow>
            <mo stretchy="false">|</mo>
          </mrow>
        </mfrac>
        <munder>
          <mo data-mjx-texclass="OP">&#x2211;</mo>
          <mrow data-mjx-texclass="ORD">
            <mi>i</mi>
            <mo>&#x2208;</mo>
            <mrow data-mjx-texclass="ORD">
              <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">B</mi>
            </mrow>
          </mrow>
        </munder>
        <msup>
          <mrow data-mjx-texclass="ORD">
            <mi mathvariant="bold">x</mi>
          </mrow>
          <mrow data-mjx-texclass="ORD">
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mrow data-mjx-texclass="INNER">
          <mo data-mjx-texclass="OPEN">(</mo>
          <msup>
            <mrow data-mjx-texclass="ORD">
              <mi mathvariant="bold">w</mi>
            </mrow>
            <mi mathvariant="normal">&#x22A4;</mi>
          </msup>
          <msup>
            <mrow data-mjx-texclass="ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mrow data-mjx-texclass="ORD">
              <mo stretchy="false">(</mo>
              <mi>i</mi>
              <mo stretchy="false">)</mo>
            </mrow>
          </msup>
          <mo>+</mo>
          <mi>b</mi>
          <mo>&#x2212;</mo>
          <msup>
            <mi>y</mi>
            <mrow data-mjx-texclass="ORD">
              <mo stretchy="false">(</mo>
              <mi>i</mi>
              <mo stretchy="false">)</mo>
            </mrow>
          </msup>
          <mo data-mjx-texclass="CLOSE">)</mo>
        </mrow>
        <mo>.</mo>
      </mtd>
    </mtr>
  </mtable>
</math>  

### 3.4 暂退法  
#### 3.4.1 再看过拟合  
当面对更多的特征而样本不足时，线性模型往往会过拟合  
相反，当给出更多样本而不是特征，通常线性模型不会过拟合  

但是线性模型没有考虑特征之间的交互作用，对于每个特征线性模型都必须指定正的或负的权重，从而忽略了其它特征  

泛化性和灵活性之间的这种基本权衡被描述为 *偏差-方差权衡*  
> 线性模型有很高的偏差：它们只能表示一小类函数  
模型的方差很低：它们在不同的随机数据样本上可以得出相似的结果  
#### 3.4.2 扰动的稳定性  
经典泛化理论认为，为了缩小训练和测试性能之间的差距，应该以简单的模型为目标  

在训练过程中，在计算后续层之前向网络的每一层注入噪声  
因为当训练一个有多层的深层网络时，注入噪声只会在输入-输出映射上增强平滑性  

这个想法被称为 *暂退法*  
暂退法在前向传播过程中，计算每一内部层的同时注入噪声，这已经成为训练神经网络的常用技术  
该种方法之所以被称为暂退法  
是因为从表面上看在训练过程中会丢弃一些神经元  

那么关键的挑战就是如何注入这种噪声  
在毕晓普的工作中，他将高斯噪声添加到线性模型的输入中  
在每次训练迭代中，他将从均值为零的分布 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x3F5;</mi>
  <mo>&#x223C;</mo>
  <mrow data-mjx-texclass="ORD">
    <mi data-mjx-variant="-tex-calligraphic" mathvariant="script">N</mi>
  </mrow>
  <mo stretchy="false">(</mo>
  <mn>0</mn>
  <mo>,</mo>
  <msup>
    <mi>&#x3C3;</mi>
    <mn>2</mn>
  </msup>
  <mo stretchy="false">)</mo>
</math>采样噪声添加到输入 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
</math>， 从而产生扰动点<math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">x</mi>
    </mrow>
    <mo data-mjx-alternate="1">&#x2032;</mo>
  </msup>
  <mo>=</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo>+</mo>
  <mi>&#x3F5;</mi>
</math> ， 预期是 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>E</mi>
  <mo stretchy="false">[</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">x</mi>
    </mrow>
    <mo data-mjx-alternate="1">&#x2032;</mo>
  </msup>
  <mo stretchy="false">]</mo>
  <mo>=</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
</math>  
在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差  
每个中间活性值 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>h</mi>
</math> 以暂退概率 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>p</mi>
</math> 由随机变量 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mi>h</mi>
    <mo data-mjx-alternate="1">&#x2032;</mo>
  </msup>
</math> 替换  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true" columnalign="right" columnspacing="0em" rowspacing="3pt">
    <mtr>
      <mtd>
        <mtable displaystyle="true" columnspacing="" rowspacing="3pt">
          <mtr>
            <mtd>
              <msup>
                <mi>h</mi>
                <mo data-mjx-alternate="1">&#x2032;</mo>
              </msup>
              <mo>=</mo>
              <mrow data-mjx-texclass="INNER">
                <mo data-mjx-texclass="OPEN">{</mo>
                <mtable columnalign="left left" columnspacing="1em" rowspacing=".2em">
                  <mtr>
                    <mtd>
                      <mn>0</mn>
                    </mtd>
                    <mtd>
                      <mtext>&#xA0;&#x6982;&#x7387;&#x4E3A;&#xA0;</mtext>
                      <mi>p</mi>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <mfrac>
                        <mi>h</mi>
                        <mrow>
                          <mn>1</mn>
                          <mo>&#x2212;</mo>
                          <mi>p</mi>
                        </mrow>
                      </mfrac>
                    </mtd>
                    <mtd>
                      <mtext>&#xA0;&#x5176;&#x4ED6;&#x60C5;&#x51B5;</mtext>
                    </mtd>
                  </mtr>
                </mtable>
                <mo data-mjx-texclass="CLOSE" fence="true" stretchy="true" symmetric="true"></mo>
              </mrow>
            </mtd>
          </mtr>
        </mtable>
      </mtd>
    </mtr>
  </mtable>
</math>  

#### 3.4.3 实践中的暂退法  
将暂退法应用到隐藏层，以 *p* 的概率将隐藏单元置为零时，结果可以看作一个只包含原始神经元子集的网络    
就如图所示 ：  
![图片](https://zh.d2l.ai/_images/dropout2.svg)  

### 3.5 正向传播、反向传播和计算图  
#### 3.5.1 正向传播  
**前向传播** 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果  
假设输入样本是 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mi>d</mi>
  </msup>
</math> ， 并且我们的隐藏层不包括偏置项  
即中间变量为 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">z</mi>
  </mrow>
  <mo>=</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo>,</mo>
</math>  

其中 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>h</mi>
      <mo>&#xD7;</mo>
      <mi>d</mi>
    </mrow>
  </msup>
</math>是隐藏层的权重参数  
将中间变量 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">z</mi>
  </mrow>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mi>h</mi>
  </msup>
</math> 通过激活函数 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x3D5;</mi>
</math> 后， 我们得到长度为 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>h</mi>
</math> 的隐藏激活向量 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">h</mi>
  </mrow>
  <mo>=</mo>
  <mi>&#x3D5;</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">z</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>.</mo>
</math>  

隐藏变量 **h** 也是一个中间变量。假设输出层的参数只有权重 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>q</mi>
      <mo>&#xD7;</mo>
      <mi>h</mi>
    </mrow>
  </msup>
</math> ， 我们可以得到输出层变量，它是一个长度为 q 的向量 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">o</mi>
  </mrow>
  <mo>=</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">h</mi>
  </mrow>
  <mo>.</mo>
</math>  

假设损失函数为 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>l</mi>
</math> ，样本标签为 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>y</mi>
</math> ，我们可以计算单个数据样本的损失项， 

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>L</mi>
  <mo>=</mo>
  <mi>l</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">o</mi>
  </mrow>
  <mo>,</mo>
  <mi>y</mi>
  <mo stretchy="false">)</mo>
  <mo>.</mo>
</math>  

根据 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msub>
    <mi>L</mi>
    <mn>2</mn>
  </msub>
</math> 正则化的定义，给定超参数 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msub>
    <mi>L</mi>
    <mn>2</mn>
  </msub>
</math> ，正则化项为  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>s</mi>
  <mo>=</mo>
  <mfrac>
    <mi>&#x3BB;</mi>
    <mn>2</mn>
  </mfrac>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">(</mo>
    <mo data-mjx-texclass="ORD" fence="false" stretchy="false">&#x2016;</mo>
    <msup>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">W</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo stretchy="false">)</mo>
      </mrow>
    </msup>
    <msubsup>
      <mo data-mjx-texclass="ORD" fence="false" stretchy="false">&#x2016;</mo>
      <mi>F</mi>
      <mn>2</mn>
    </msubsup>
    <mo>+</mo>
    <mo data-mjx-texclass="ORD" fence="false" stretchy="false">&#x2016;</mo>
    <msup>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">W</mi>
      </mrow>
      <mrow data-mjx-texclass="ORD">
        <mo stretchy="false">(</mo>
        <mn>2</mn>
        <mo stretchy="false">)</mo>
      </mrow>
    </msup>
    <msubsup>
      <mo data-mjx-texclass="ORD" fence="false" stretchy="false">&#x2016;</mo>
      <mi>F</mi>
      <mn>2</mn>
    </msubsup>
    <mo data-mjx-texclass="CLOSE">)</mo>
  </mrow>
  <mo>,</mo>
</math>  

最后，模型在给定数据样本上的正则化损失为 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>J</mi>
  <mo>=</mo>
  <mi>L</mi>
  <mo>+</mo>
  <mi>s</mi>
  <mo>.</mo>
</math>  

在下面的讨论中，我们 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>J</mi>
</math> 将称为目标函数  

#### 3.5.2 正向传播计算图  
![图片](https://zh.d2l.ai/_images/forward.svg)  

#### 3.5.3 反向传播  
反向传播指的是计算神经网络参数梯度的方法  
该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络  

假设我们有函数 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="sans-serif">Y</mi>
  </mrow>
  <mo>=</mo>
  <mi>f</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="sans-serif">X</mi>
  </mrow>
  <mo stretchy="false">)</mo>
</math> 和 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="sans-serif">Z</mi>
  </mrow>
  <mo>=</mo>
  <mi>g</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="sans-serif">Y</mi>
  </mrow>
  <mo stretchy="false">)</mo>
</math> ， 其中输入和输出 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="sans-serif">X</mi>
  </mrow>
  <mo>,</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="sans-serif">Y</mi>
  </mrow>
  <mo>,</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="sans-serif">Z</mi>
  </mrow>
</math> 是任意形状的张量  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mfrac>
    <mrow>
      <mi>&#x2202;</mi>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="sans-serif">Z</mi>
      </mrow>
    </mrow>
    <mrow>
      <mi>&#x2202;</mi>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="sans-serif">X</mi>
      </mrow>
    </mrow>
  </mfrac>
  <mo>=</mo>
  <mtext>prod</mtext>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">(</mo>
    <mfrac>
      <mrow>
        <mi>&#x2202;</mi>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="sans-serif">Z</mi>
        </mrow>
      </mrow>
      <mrow>
        <mi>&#x2202;</mi>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="sans-serif">Y</mi>
        </mrow>
      </mrow>
    </mfrac>
    <mo>,</mo>
    <mfrac>
      <mrow>
        <mi>&#x2202;</mi>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="sans-serif">Y</mi>
        </mrow>
      </mrow>
      <mrow>
        <mi>&#x2202;</mi>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="sans-serif">X</mi>
        </mrow>
      </mrow>
    </mfrac>
    <mo data-mjx-texclass="CLOSE">)</mo>
  </mrow>
  <mo>.</mo>
</math>  


计算顺序与正向传播中执行的顺序相反，因为要从计算图的结果开始，并朝着参数的方向努力  
第一步是计算目标函数 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>J</mi>
  <mo>=</mo>
  <mi>L</mi>
  <mo>+</mo>
  <mi>s</mi>
</math> 相对于损失项 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>L</mi>
</math> 和正则项 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>s</mi>
</math> 的梯度  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mfrac>
    <mrow>
      <mi>&#x2202;</mi>
      <mi>J</mi>
    </mrow>
    <mrow>
      <mi>&#x2202;</mi>
      <mi>L</mi>
    </mrow>
  </mfrac>
  <mo>=</mo>
  <mn>1</mn>
  <mstyle scriptlevel="0">
    <mspace width="0.278em"></mspace>
  </mstyle>
  <mtext>and</mtext>
  <mstyle scriptlevel="0">
    <mspace width="0.278em"></mspace>
  </mstyle>
  <mfrac>
    <mrow>
      <mi>&#x2202;</mi>
      <mi>J</mi>
    </mrow>
    <mrow>
      <mi>&#x2202;</mi>
      <mi>s</mi>
    </mrow>
  </mfrac>
  <mo>=</mo>
  <mn>1.</mn>
</math>  

接下来，根据链式法则计算目标函数关于输出层变量 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">o</mi>
  </mrow>
</math> 的梯度 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mfrac>
    <mrow>
      <mi>&#x2202;</mi>
      <mi>J</mi>
    </mrow>
    <mrow>
      <mi>&#x2202;</mi>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">o</mi>
      </mrow>
    </mrow>
  </mfrac>
  <mo>=</mo>
  <mtext>prod</mtext>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">(</mo>
    <mfrac>
      <mrow>
        <mi>&#x2202;</mi>
        <mi>J</mi>
      </mrow>
      <mrow>
        <mi>&#x2202;</mi>
        <mi>L</mi>
      </mrow>
    </mfrac>
    <mo>,</mo>
    <mfrac>
      <mrow>
        <mi>&#x2202;</mi>
        <mi>L</mi>
      </mrow>
      <mrow>
        <mi>&#x2202;</mi>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">o</mi>
        </mrow>
      </mrow>
    </mfrac>
    <mo data-mjx-texclass="CLOSE">)</mo>
  </mrow>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>&#x2202;</mi>
      <mi>L</mi>
    </mrow>
    <mrow>
      <mi>&#x2202;</mi>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">o</mi>
      </mrow>
    </mrow>
  </mfrac>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mi>q</mi>
  </msup>
  <mo>.</mo>
</math>  

接下来，计算正则化项相对于两个参数的梯度 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mfrac>
    <mrow>
      <mi>&#x2202;</mi>
      <mi>s</mi>
    </mrow>
    <mrow>
      <mi>&#x2202;</mi>
      <msup>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">W</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mo stretchy="false">(</mo>
          <mn>1</mn>
          <mo stretchy="false">)</mo>
        </mrow>
      </msup>
    </mrow>
  </mfrac>
  <mo>=</mo>
  <mi>&#x3BB;</mi>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mstyle scriptlevel="0">
    <mspace width="0.278em"></mspace>
  </mstyle>
  <mtext>and</mtext>
  <mstyle scriptlevel="0">
    <mspace width="0.278em"></mspace>
  </mstyle>
  <mfrac>
    <mrow>
      <mi>&#x2202;</mi>
      <mi>s</mi>
    </mrow>
    <mrow>
      <mi>&#x2202;</mi>
      <msup>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">W</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mo stretchy="false">(</mo>
          <mn>2</mn>
          <mo stretchy="false">)</mo>
        </mrow>
      </msup>
    </mrow>
  </mfrac>
  <mo>=</mo>
  <mi>&#x3BB;</mi>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>.</mo>
</math>  

可以计算最接近输出层的模型参数的梯度 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x2202;</mi>
  <mi>J</mi>
  <mrow data-mjx-texclass="ORD">
    <mo>/</mo>
  </mrow>
  <mi>&#x2202;</mi>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>q</mi>
      <mo>&#xD7;</mo>
      <mi>h</mi>
    </mrow>
  </msup>
</math>

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mfrac>
    <mrow>
      <mi>&#x2202;</mi>
      <mi>J</mi>
    </mrow>
    <mrow>
      <mi>&#x2202;</mi>
      <msup>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">W</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mo stretchy="false">(</mo>
          <mn>2</mn>
          <mo stretchy="false">)</mo>
        </mrow>
      </msup>
    </mrow>
  </mfrac>
  <mo>=</mo>
  <mtext>prod</mtext>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">(</mo>
    <mfrac>
      <mrow>
        <mi>&#x2202;</mi>
        <mi>J</mi>
      </mrow>
      <mrow>
        <mi>&#x2202;</mi>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">o</mi>
        </mrow>
      </mrow>
    </mfrac>
    <mo>,</mo>
    <mfrac>
      <mrow>
        <mi>&#x2202;</mi>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">o</mi>
        </mrow>
      </mrow>
      <mrow>
        <mi>&#x2202;</mi>
        <msup>
          <mrow data-mjx-texclass="ORD">
            <mi mathvariant="bold">W</mi>
          </mrow>
          <mrow data-mjx-texclass="ORD">
            <mo stretchy="false">(</mo>
            <mn>2</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
      </mrow>
    </mfrac>
    <mo data-mjx-texclass="CLOSE">)</mo>
  </mrow>
  <mo>+</mo>
  <mtext>prod</mtext>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">(</mo>
    <mfrac>
      <mrow>
        <mi>&#x2202;</mi>
        <mi>J</mi>
      </mrow>
      <mrow>
        <mi>&#x2202;</mi>
        <mi>s</mi>
      </mrow>
    </mfrac>
    <mo>,</mo>
    <mfrac>
      <mrow>
        <mi>&#x2202;</mi>
        <mi>s</mi>
      </mrow>
      <mrow>
        <mi>&#x2202;</mi>
        <msup>
          <mrow data-mjx-texclass="ORD">
            <mi mathvariant="bold">W</mi>
          </mrow>
          <mrow data-mjx-texclass="ORD">
            <mo stretchy="false">(</mo>
            <mn>2</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
      </mrow>
    </mfrac>
    <mo data-mjx-texclass="CLOSE">)</mo>
  </mrow>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>&#x2202;</mi>
      <mi>J</mi>
    </mrow>
    <mrow>
      <mi>&#x2202;</mi>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">o</mi>
      </mrow>
    </mrow>
  </mfrac>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">h</mi>
    </mrow>
    <mi mathvariant="normal">&#x22A4;</mi>
  </msup>
  <mo>+</mo>
  <mi>&#x3BB;</mi>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>2</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>.</mo>
</math>  

为了获得关于 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
</math> 的梯度，需要继续沿着输出层到隐藏层反向传播  
关于隐藏层输出的梯度 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x2202;</mi>
  <mi>J</mi>
  <mrow data-mjx-texclass="ORD">
    <mo>/</mo>
  </mrow>
  <mi>&#x2202;</mi>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">h</mi>
  </mrow>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mi>h</mi>
  </msup>
</math> 由下式给出 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mfrac>
    <mrow>
      <mi>&#x2202;</mi>
      <mi>J</mi>
    </mrow>
    <mrow>
      <mi>&#x2202;</mi>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">h</mi>
      </mrow>
    </mrow>
  </mfrac>
  <mo>=</mo>
  <mtext>prod</mtext>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">(</mo>
    <mfrac>
      <mrow>
        <mi>&#x2202;</mi>
        <mi>J</mi>
      </mrow>
      <mrow>
        <mi>&#x2202;</mi>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">o</mi>
        </mrow>
      </mrow>
    </mfrac>
    <mo>,</mo>
    <mfrac>
      <mrow>
        <mi>&#x2202;</mi>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">o</mi>
        </mrow>
      </mrow>
      <mrow>
        <mi>&#x2202;</mi>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">h</mi>
        </mrow>
      </mrow>
    </mfrac>
    <mo data-mjx-texclass="CLOSE">)</mo>
  </mrow>
  <mo>=</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <msup>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">W</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mo stretchy="false">(</mo>
          <mn>2</mn>
          <mo stretchy="false">)</mo>
        </mrow>
      </msup>
    </mrow>
    <mi mathvariant="normal">&#x22A4;</mi>
  </msup>
  <mfrac>
    <mrow>
      <mi>&#x2202;</mi>
      <mi>J</mi>
    </mrow>
    <mrow>
      <mi>&#x2202;</mi>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">o</mi>
      </mrow>
    </mrow>
  </mfrac>
  <mo>.</mo>
</math>  

由于激活函数 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x3D5;</mi>
</math> 是按元素计算的，计算中间 **z** 变量的梯度 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x2202;</mi>
  <mi>J</mi>
  <mrow data-mjx-texclass="ORD">
    <mo>/</mo>
  </mrow>
  <mi>&#x2202;</mi>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">z</mi>
  </mrow>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mi>h</mi>
  </msup>
</math> 需要使用按元素乘法运算符，我们用 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mo>&#x2299;</mo>
</math> 表示 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mfrac>
    <mrow>
      <mi>&#x2202;</mi>
      <mi>J</mi>
    </mrow>
    <mrow>
      <mi>&#x2202;</mi>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">z</mi>
      </mrow>
    </mrow>
  </mfrac>
  <mo>=</mo>
  <mtext>prod</mtext>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">(</mo>
    <mfrac>
      <mrow>
        <mi>&#x2202;</mi>
        <mi>J</mi>
      </mrow>
      <mrow>
        <mi>&#x2202;</mi>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">h</mi>
        </mrow>
      </mrow>
    </mfrac>
    <mo>,</mo>
    <mfrac>
      <mrow>
        <mi>&#x2202;</mi>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">h</mi>
        </mrow>
      </mrow>
      <mrow>
        <mi>&#x2202;</mi>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">z</mi>
        </mrow>
      </mrow>
    </mfrac>
    <mo data-mjx-texclass="CLOSE">)</mo>
  </mrow>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>&#x2202;</mi>
      <mi>J</mi>
    </mrow>
    <mrow>
      <mi>&#x2202;</mi>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">h</mi>
      </mrow>
    </mrow>
  </mfrac>
  <mo>&#x2299;</mo>
  <msup>
    <mi>&#x3D5;</mi>
    <mo data-mjx-alternate="1">&#x2032;</mo>
  </msup>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">(</mo>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">z</mi>
    </mrow>
    <mo data-mjx-texclass="CLOSE">)</mo>
  </mrow>
  <mo>.</mo>
</math>   

最后，可以得到最接近输入层的模型参数的梯度 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>&#x2202;</mi>
  <mi>J</mi>
  <mrow data-mjx-texclass="ORD">
    <mo>/</mo>
  </mrow>
  <mi>&#x2202;</mi>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>&#x2208;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="double-struck">R</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>h</mi>
      <mo>&#xD7;</mo>
      <mi>d</mi>
    </mrow>
  </msup>
</math> ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mfrac>
    <mrow>
      <mi>&#x2202;</mi>
      <mi>J</mi>
    </mrow>
    <mrow>
      <mi>&#x2202;</mi>
      <msup>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">W</mi>
        </mrow>
        <mrow data-mjx-texclass="ORD">
          <mo stretchy="false">(</mo>
          <mn>1</mn>
          <mo stretchy="false">)</mo>
        </mrow>
      </msup>
    </mrow>
  </mfrac>
  <mo>=</mo>
  <mtext>prod</mtext>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">(</mo>
    <mfrac>
      <mrow>
        <mi>&#x2202;</mi>
        <mi>J</mi>
      </mrow>
      <mrow>
        <mi>&#x2202;</mi>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">z</mi>
        </mrow>
      </mrow>
    </mfrac>
    <mo>,</mo>
    <mfrac>
      <mrow>
        <mi>&#x2202;</mi>
        <mrow data-mjx-texclass="ORD">
          <mi mathvariant="bold">z</mi>
        </mrow>
      </mrow>
      <mrow>
        <mi>&#x2202;</mi>
        <msup>
          <mrow data-mjx-texclass="ORD">
            <mi mathvariant="bold">W</mi>
          </mrow>
          <mrow data-mjx-texclass="ORD">
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
      </mrow>
    </mfrac>
    <mo data-mjx-texclass="CLOSE">)</mo>
  </mrow>
  <mo>+</mo>
  <mtext>prod</mtext>
  <mrow data-mjx-texclass="INNER">
    <mo data-mjx-texclass="OPEN">(</mo>
    <mfrac>
      <mrow>
        <mi>&#x2202;</mi>
        <mi>J</mi>
      </mrow>
      <mrow>
        <mi>&#x2202;</mi>
        <mi>s</mi>
      </mrow>
    </mfrac>
    <mo>,</mo>
    <mfrac>
      <mrow>
        <mi>&#x2202;</mi>
        <mi>s</mi>
      </mrow>
      <mrow>
        <mi>&#x2202;</mi>
        <msup>
          <mrow data-mjx-texclass="ORD">
            <mi mathvariant="bold">W</mi>
          </mrow>
          <mrow data-mjx-texclass="ORD">
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
      </mrow>
    </mfrac>
    <mo data-mjx-texclass="CLOSE">)</mo>
  </mrow>
  <mo>=</mo>
  <mfrac>
    <mrow>
      <mi>&#x2202;</mi>
      <mi>J</mi>
    </mrow>
    <mrow>
      <mi>&#x2202;</mi>
      <mrow data-mjx-texclass="ORD">
        <mi mathvariant="bold">z</mi>
      </mrow>
    </mrow>
  </mfrac>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">x</mi>
    </mrow>
    <mi mathvariant="normal">&#x22A4;</mi>
  </msup>
  <mo>+</mo>
  <mi>&#x3BB;</mi>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">W</mi>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mo stretchy="false">(</mo>
      <mn>1</mn>
      <mo stretchy="false">)</mo>
    </mrow>
  </msup>
  <mo>.</mo>
</math>  

#### 3.5.4 训练神经网络  
在训练神经网络时，正向传播和反向传播相互依赖  

### 3.6 数值稳定性和模型初始化  
#### 3.6.1 梯度消失与梯度爆炸  
不稳定梯度带来的风险不止在于数值表示；不稳定梯度也威胁到优化算法的稳定性  

* **梯度爆炸**问题 ：参数更新过大，破坏了模型的稳定收敛  
* **梯度消失**问题： 参数更新过小，在每次更新时几乎不会移动，导致模型无法学习  

##### 3.6.1.1 梯度消失  
之前提过的 sigmoid 函数就存在这种问题  
![图片](https://zh.d2l.ai/_images/output_numerical-stability-and-init_e60514_6_0.svg)  
正如上图，当sigmoid函数的输入很大或是很小时，它的梯度都会消失  

##### 3.6.1.2 梯度爆炸  
为了更好地说明这一点，生成100个高斯随机矩阵，并将它们与某个初始矩阵相乘  
对于我们选择的尺度（方差 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mi>&#x3C3;</mi>
    <mn>2</mn>
  </msup>
  <mo>=</mo>
  <mn>1</mn>
</math> ），矩阵乘积发生爆炸  

```python 
一个矩阵
 tensor([[-0.7872,  2.7090,  0.5996, -1.3191],
        [-1.8260, -0.7130, -0.5521,  0.1051],
        [ 1.1213,  1.0472, -0.3991, -0.3802],
        [ 0.5552,  0.4517, -0.3218,  0.5214]])
乘以100个矩阵后
 tensor([[-2.1897e+26,  8.8308e+26,  1.9813e+26,  1.7019e+26],
        [ 1.3110e+26, -5.2870e+26, -1.1862e+26, -1.0189e+26],
        [-1.6008e+26,  6.4559e+26,  1.4485e+26,  1.2442e+26],
        [ 3.0943e+25, -1.2479e+26, -2.7998e+25, -2.4050e+25]])
```  

#### 3.6.2 参数初始化  
##### 3.6.2.1 默认初始化  
##### 3.6.2.2 Xavier 初始化  
满足 ：  

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true" columnalign="right" columnspacing="" rowspacing="3pt">
    <mtr>
      <mtd>
        <mfrac>
          <mn>1</mn>
          <mn>2</mn>
        </mfrac>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>n</mi>
          <mrow data-mjx-texclass="ORD">
            <mi data-mjx-auto-op="false">in</mi>
          </mrow>
        </msub>
        <mo>+</mo>
        <msub>
          <mi>n</mi>
          <mrow data-mjx-texclass="ORD">
            <mi data-mjx-auto-op="false">out</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <msup>
          <mi>&#x3C3;</mi>
          <mn>2</mn>
        </msup>
        <mo>=</mo>
        <mn>1</mn>
        <mtext>&#xA0;&#x6216;&#x7B49;&#x4EF7;&#x4E8E;&#xA0;</mtext>
        <mi>&#x3C3;</mi>
        <mo>=</mo>
        <msqrt>
          <mfrac>
            <mn>2</mn>
            <mrow>
              <msub>
                <mi>n</mi>
                <mrow data-mjx-texclass="ORD">
                  <mi data-mjx-auto-op="false">in</mi>
                </mrow>
              </msub>
              <mo>+</mo>
              <msub>
                <mi>n</mi>
                <mrow data-mjx-texclass="ORD">
                  <mi data-mjx-auto-op="false">out</mi>
                </mrow>
              </msub>
            </mrow>
          </mfrac>
        </msqrt>
        <mo>.</mo>
      </mtd>
    </mtr>
  </mtable>
</math>  
这就是现在标准且实用的Xavier初始化的基础  
通常，Xavier初始化从均值为零，方差 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <msup>
    <mi>&#x3C3;</mi>
    <mn>2</mn>
  </msup>
  <mo>=</mo>
  <mfrac>
    <mn>2</mn>
    <mrow>
      <msub>
        <mi>n</mi>
        <mrow data-mjx-texclass="ORD">
          <mi data-mjx-auto-op="false">in</mi>
        </mrow>
      </msub>
      <mo>+</mo>
      <msub>
        <mi>n</mi>
        <mrow data-mjx-texclass="ORD">
          <mi data-mjx-auto-op="false">out</mi>
        </mrow>
      </msub>
    </mrow>
  </mfrac>
</math> 的高斯分布中采样权重  

## 4. 深度学习计算  
### 4.1 层和块  
单个神经网络  

* 接受一些输入  
* 生成相应的标量输出   
* 具有一组相关参数 ，更新这些参数可以优化某目标函数  

当考虑具有多个输出的网络时，利用矢量化算法来描述整层神经元  

* 接受一组输入  
* 生成相应的输出  
* 由一组可调整参数描述  

![图片](https://zh.d2l.ai/_images/blocks.svg)  

#### 4.1.1 自定义块  
```python  
class MLP(nn.Module):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用MLP的父类Module的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层

    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def forward(self, X):
        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(X)))  
```  
可以试一下这个函数 ：  
```python
net = MLP()
net(x)
```  
```python
tensor([[ 0.0669,  0.2202, -0.0912, -0.0064,  0.1474, -0.0577, -0.3006,  0.1256,
         -0.0280,  0.4040],
        [ 0.0545,  0.2591, -0.0297,  0.1141,  0.1887,  0.0094, -0.2686,  0.0732,
         -0.0135,  0.3865]], grad_fn=<AddmmBackward0>)
```  

块的一个主要优点是它的多功能性  
可以子类化块以创建层（如全连接层的类）、 整个模型（如上面的MLP类）或具有中等复杂度的各种组件  

#### 4.1.2 顺序块  
只需要定义两个关键函数 ：  

> 一种将块逐个追加到列表中的函数  
一种前向传播函数，用于将输入按追加块的顺序传递给块组成的“链条”  

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员
            # 变量_modules中。_module的类型是OrderedDict
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X)
        return X
```  

__init__函数将每个模块逐个添加到有序字典_modules中  
_modules的主要优点是：在模块的参数初始化过程中，系统知道在_modules字典中查找需要初始化参数的子块  

当MySequential的前向传播函数被调用时， 每个添加的块都按照它们被添加的顺序执行  

```python
net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
net(X)
```  
```python  
tensor([[ 2.2759e-01, -4.7003e-02,  4.2846e-01, -1.2546e-01,  1.5296e-01,
          1.8972e-01,  9.7048e-02,  4.5479e-04, -3.7986e-02,  6.4842e-02],
        [ 2.7825e-01, -9.7517e-02,  4.8541e-01, -2.4519e-01, -8.4580e-02,
          2.8538e-01,  3.6861e-02,  2.9411e-02, -1.0612e-01,  1.2620e-01]],
       grad_fn=<AddmmBackward0>)
```  

#### 4.1.3 在正向传播函数中执行代码  
需要一个计算函数 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>f</mi>
  <mo stretchy="false">(</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
  <mo>,</mo>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">w</mi>
  </mrow>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mi>c</mi>
  <mo>&#x22C5;</mo>
  <msup>
    <mrow data-mjx-texclass="ORD">
      <mi mathvariant="bold">w</mi>
    </mrow>
    <mi mathvariant="normal">&#x22A4;</mi>
  </msup>
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
</math> 的层， 其中 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">x</mi>
  </mrow>
</math> 是输入， <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow data-mjx-texclass="ORD">
    <mi mathvariant="bold">w</mi>
  </mrow>
</math> 是参数， <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>c</mi>
</math> 是某个在优化过程中没有更新的指定常量时   
因此实现了一个FixedHiddenMLP类 ：  
```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 不计算梯度的随机权重参数。因此其在训练期间保持不变
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        # 使用创建的常量参数以及relu和mm函数
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        # 复用全连接层。这相当于两个全连接层共享参数
        X = self.linear(X)
        # 控制流
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()
```  

在FixedHiddenMLP模型中，实现了一个隐藏层，其权重在实例化时被随机初始化，之后为常量  
这个权重不是一个模型参数，因此它永远不会被反向传播更新  
然后，神经网络将这个固定层的输出通过一个全连接层  
```python
net = FixedHiddenMLP()
net(X)
```  

```python 
tensor(0.1862, grad_fn=<SumBackward0>)  
```  

### 4.2 参数管理  
本节，我们将介绍以下内容 ：  

* 访问参数，用于调试、诊断和可视化  
* 参数初始化  
* 在不同模型组件间共享参数  

#### 4.2.1 参数访问  
首先，这个全连接层包含两个参数，分别是该层的权重和偏置  
两者都储存单精度浮点数  
**注意** ： 参数名称允许唯一标识每个参数，即使在包含数百个层的网络中也是如此  
##### 4.2.1.1 目标参数  
每个参数都表示为参数类的一个实例  
要对参数执行任何操作，首先我们需要访问底层的数值  
下面的代码从第二个全连接层（即第三个神经网络层）提取偏置，提取后返回的是一个参数类实例，并进一步访问该参数的值  
```python
print(type(net[1].bias))
print(net[1].bias)
print(net[1].bias.data()) 
```  

参数是复合的对象，包含值、梯度和额外信息  
这就是我们需要显式参数值的原因  
除了值之外，我们还可以访问每个参数的梯度  
##### 4.2.1.2 一次性访问所有参数  
需要对所有参数执行操作时，逐个访问它们可能会很麻烦  
处理更复杂的块（例如，嵌套块）时，情况可能会变得特别复杂， 因为我们需要递归整个树来提取每个子块的参数  
下面，通过演示来比较访问第一个全连接层的参数和访问所有层 ：  
```python
print(*[(name, param.shape) for name, param in net[0].named_parameters()])
print(*[(name, param.shape) for name, param in net.named_parameters()])

```  
```python
('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))
('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))
```  

##### 4.2.1.3 从嵌套块收集参数  
定义一个生成块的函数（可以说是“块工厂”），然后将这些块组合到更大的块中  
```python
def block1():
    return nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                         nn.Linear(8, 4), nn.ReLU())

def block2():
    net = nn.Sequential()
    for i in range(4):
        # 在这里嵌套
        net.add_module(f'block {i}', block1())
    return net

rgnet = nn.Sequential(block2(), nn.Linear(4, 1))
rgnet(X)

```
```python
tensor([[0.2596],
        [0.2596]], grad_fn=<AddmmBackward0>)
```  
#### 4.2.2 参数初始化  
##### 4.2.2.1 内置初始化  
首先调用内置的初始化器  
```python
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
net.apply(init_normal)
net[0].weight.data[0], net[0].bias.data[0]
``` 
##### 4.2.2.2 自定义初始化  
在下面的例子中，我们使用以下的分布为任意权重参数 <math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>w</mi>
</math> 定义初始化方法 ：  
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mtable displaystyle="true" columnalign="right" columnspacing="0em" rowspacing="3pt">
    <mtr>
      <mtd>
        <mtable displaystyle="true" columnspacing="" rowspacing="3pt">
          <mtr>
            <mtd>
              <mi>w</mi>
              <mo>&#x223C;</mo>
              <mrow data-mjx-texclass="INNER">
                <mo data-mjx-texclass="OPEN">{</mo>
                <mtable columnalign="left left" columnspacing="1em" rowspacing=".2em">
                  <mtr>
                    <mtd>
                      <mi>U</mi>
                      <mo stretchy="false">(</mo>
                      <mn>5</mn>
                      <mo>,</mo>
                      <mn>10</mn>
                      <mo stretchy="false">)</mo>
                    </mtd>
                    <mtd>
                      <mtext>&#xA0;&#x53EF;&#x80FD;&#x6027;&#xA0;</mtext>
                      <mfrac>
                        <mn>1</mn>
                        <mn>4</mn>
                      </mfrac>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <mn>0</mn>
                    </mtd>
                    <mtd>
                      <mtext>&#xA0;&#x53EF;&#x80FD;&#x6027;&#xA0;</mtext>
                      <mfrac>
                        <mn>1</mn>
                        <mn>2</mn>
                      </mfrac>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd>
                      <mi>U</mi>
                      <mo stretchy="false">(</mo>
                      <mo>&#x2212;</mo>
                      <mn>10</mn>
                      <mo>,</mo>
                      <mo>&#x2212;</mo>
                      <mn>5</mn>
                      <mo stretchy="false">)</mo>
                    </mtd>
                    <mtd>
                      <mtext>&#xA0;&#x53EF;&#x80FD;&#x6027;&#xA0;</mtext>
                      <mfrac>
                        <mn>1</mn>
                        <mn>4</mn>
                      </mfrac>
                    </mtd>
                  </mtr>
                </mtable>
                <mo data-mjx-texclass="CLOSE" fence="true" stretchy="true" symmetric="true"></mo>
              </mrow>
            </mtd>
          </mtr>
        </mtable>
      </mtd>
    </mtr>
  </mtable>
</math>  

实现了一个my_init函数来应用到net  
```python
def my_init(m):
    if type(m) == nn.Linear:
        print("Init", *[(name, param.shape)
                        for name, param in m.named_parameters()][0])
        nn.init.uniform_(m.weight, -10, 10)
        m.weight.data *= m.weight.data.abs() >= 5

net.apply(my_init)
net[0].weight[:2]
```  
```python
Init weight torch.Size([8, 4])
Init weight torch.Size([1, 8])
```  
#### 4.2.3 参数绑定  
希望在多个层间共享参数：可以定义一个稠密层，然后使用它的参数来设置另一个层的参数  
```python
# 我们需要给共享层一个名称，以便可以引用它的参数
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8, 1))
net(X)
# 检查参数是否相同
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0, 0] = 100
# 确保它们实际上是同一个对象，而不只是有相同的值
print(net[2].weight.data[0] == net[4].weight.data[0])
```  
### 4.3 延后初始化  
### 4.4 自定义层  
通过自定义层增加神经网络的灵活性  
#### 4.4.1 不带参数的层  
构造一个没有任何参数的自定义层  
要构建它，我们只需继承基础层类并实现前向传播功能  
```python
import torch
import torch.nn.functional as F
from torch import nn


class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, X):
        return X - X.mean()
```  
#### 4.4.2 带参数的层  
可以使用内置函数来创建参数，这些函数提供一些基本的管理功能  
```python
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
```  
### 4.5 读写文件  
#### 4.5.1 加载与保存张量  
对于单个张量，可以直接调用load和save函数分别读写它们  
这两个函数都要求我们提供一个名称，save要求将要保存的变量作为输入 ：  
```python
import torch
from torch import nn
from torch.nn import functional as F

x = torch.arange(4)
torch.save(x, 'x-file')
```  
现在可以将存储在文件中的数据读回内存  
```python
x2 = torch.load('x-file')
x2
```  
可以存储一个张量列表，然后把它们读回内存  
```python  
y = torch.zeros(4)
torch.save([x, y],'x-files')
x2, y2 = torch.load('x-files')
(x2, y2)
```  
可以写入或读取从字符串映射到张量的字典  
```python  
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')
mydict2
```  
#### 4.5.2 加载与保存模型参数  
```python  
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)

    def forward(self, x):
        return self.output(F.relu(self.hidden(x)))

net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)
```  
将模型的参数存储在一个叫做“mlp.params”的文件中  
```python
torch.save(net.state_dict(), 'mlp.params')
```  
